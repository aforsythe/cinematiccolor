\chapter{Workflow}

This fully computer-generated image touches upon many modern techniques in color management, including a scene-referred linear approach to rendering, shading, and illumination, in addition to on-set lighting reconstruction and texture management. 
Visual effects by Sony Pictures Imageworks. Images from “The Amazing Spider-Man” Courtesy of Columbia Pictures. © 2012 Columbia Pictures Industries, Inc. All rights reserved.

This section will describe techniques, choices and considerations that go into modern color workflows for live-action, visual effects, animation and games productions. The section starts with the introduction of two subjects that will be of use throughout the rest of the chapter: the Academy Color Encoding System and the process of crafting and managing the Look of a production. The rest of the chapter will follow an order roughly driven by the chronological order of work on a live-action visual effects movie, starting with On-Set camera, Lighting and Reference Capture, moving on to the phases of the computer graphics pipeline used in visual effects, animation and games including Texture Painting, Matte Painting, Lighting, Rendering and Compositing and finishing with Grading, Monitoring and Mastering.

To give the discussion context, a Color Pipeline will be defined as the set of all color transformations and encodings used during a motion-picture production.

A well-defined computer-graphics color pipeline rigorously manages the color transformations applied at each stage of the process, in addition to the transforms required for accurate image preview.
In a properly color-managed pipeline, the color encodings for all image inputs, image outputs, intermediate representations, displays and exports are carefully tracked. The transforms used to convert between color encodings are also rigorously defined and tracked. A well-managed pipeline protects the maximum color data available and uses the fewest transforms possible.  

\section{Academy Color Encoding System (ACES)}

The Academy of Motion Picture Arts and Sciences has defined a standard for scene-referred linear floating-point workflow components and best practices, the Academy Color Encoding System (ACES), which is in line with many of the approaches outlined in this document. ACES-based workflows define a scene-referred working space, and a reference viewing transform, in order to standardize floating-point linear interchange for moving image content in general and an archival master for motion pictures in particular.
As stated in the introduction, ACES presents an excellent embodiment of a general model of color processing that will be mirrored in most of the examples discussed in this paper. ACES will be referenced throughout this document because it is open, well-documented and widely available. It provides a great starting point for exploration. This should not be taken as a suggestion that ACES is the only option. The decisions about which color transforms and spaces to use should be based on the requirements and constraints of a given project.

\subsection{Input Transforms}

Input Transforms, previously referred to as Input Device Transforms or IDTs, for specific cameras, convert input colorimetry to a scene-referred linear wide gamut working space, ACES2065-1. Camera manufacturers typically publish Input Transforms for their specific encoding gamuts and transfer functions.

Film scans are a special case, where the Academy has defined a new density encoding standard, the Academy Density Exchange Encoding (ADX), which is stored in a DPX file. Both 10- and 16-bit variants of ADX are provided, which encode for different negative density ranges. The Transform from ADX to ACES2026-1 is not film-stock specific, but uses a generic film input linearization. The Academy publishes details of the transforms from Status M to ADX for a variety of stocks in Annex C of Technical Bulletin TB-2014-005.

\subsection{Working Space}

At the core of ACES is the Academy Color Encoding Specification, called ACES2065-1 for short. This color space is a high-dynamic range, scene-referred linear space with a color gamut that encompasses that of the human visual system. The color gamut is referred to as AP0. ACES2065-1 is a relative, rather than absolute, encoding and codes exposed middle grey as 0.18. When stored on disk, ACES files are stored in a constrained version of the OpenEXR (EXR) format, with an ACES Container metadata flag set to identify them as such. Standard ACES files are uncompressed half-float, but processing in ACES is not necessarily limited to half-float.

In the ACES workflow, the intent is for artistic changes to be made by manipulating the scene-referred linear ACES data, as viewed through an Output Transform. 

The two ACES working gamuts - AP0 and AP1
While the linear ACES2065-1 space is used for storage and interchange of ACES images, it is not an ideal working space for all operations. Because the primaries of ACES2065-1 (AP0) fall considerably outside the spectral locus in order to enclose it entirely, the results of using this as an RGB space for CGI rendering are less than ideal mathematically and for the user experience. See Section 3.4.6.2 for a more in-depth discussion. A subsidiary color space, called ACEScg, is defined in ACES to address these issues. ACEScg’s primaries, known as the AP1 gamut, fall only just outside the spectral locus. This allows CG renders and compositing operations to behave in a more intuitive manner.
     
Figure X, The ACEScc and ACEScct logarithmic encoding curves

Because the math of color grading operations likewise does not always work intuitively with linear data, ACES defines a pure log working space, ACEScc, with AP1 primaries, for grading use. A variant with a toe, ACEScct, was added later and reacts to grading operations in a manner similar to camera log curves, giving the colorist a familiar feel to the controls.

ACEScg, ACEScc and ACEScct are intended only to be transient internal working spaces for 3D, compositing and grading systems. None of them are intended to be written to files, although some facilities do use ACEScg-encoded files for interchange. Care should be taken if doing this as image metadata specifying the stored color space is notoriously unreliable. This was the central motive for having only a single defined format for storage on disk, the ACES OpenEXR container (ACES2065-4, specified by SMPTE ST.2065-4). An ACEScg file incorrectly treated as if it were an ACES2065-1 file will not yield the intended result.

\subsection{Look Transform}

In ACES, the look of a project can be defined by a Look Transform, previously referred to as a Look Modification Transform, or LMT, applied before the Output Transform. The advantage of this approach is that technically the Look Transform should not have to change regardless of whether the Output Transform chosen is for a sRGB desktop monitor, a DCI-P3 projector or an HDR TV. In practice, a slight trim is often used to compensate for different aesthetic choices between display gamuts. ACEScc and ACEScct are most commonly used as the space for crafting Looks within ACES projects. A Look Transform based on a grade in ACEScc or ACEScct should, according to the specification, include a transform to that space from ACES2065-1 and a transform back to ACES2065-1 at the end. Implementations may apply the transform directly in the working space, such as ACEScg, rather than in ACES2065-1.

The Academy Technical Bulletin TB-2014-10 describes the application of a Look Transform using the Academy Common LUT Format (CLF). Since CLF is not yet widely supported, any look applied using a LUT or grading operation which spans an entire scene or show, and is used in series with per-shot adjustments can be considered to be a Look Transform.

\subsection{Output Transform (RRT and ODT)}

The Academy also defines Output Transforms, the transform necessary for viewing ACES image data on different classes of displays.  An Output Transform is the concatenation of two transforms. First, the Reference Rendering Transform (RRT) applies a local contrast boost and tone maps scene-referred linear imagery for the purposes of picture rendering. See Section 3.2.1.1 for a more in-depth discussion of transforming scene-referred linear imagery for display. Second, the Output Device Transform (ODT). It is used to provide gamut mapping and further tone mapping to the target output device. The RRT portion is constant across all displays. The ODT varies between classes of display devices. Output Transforms are provided for common display specifications such as sRGB, Rec. 709, DCI-P3, X’Y’Z’ and ST.2084 1000, 2000 and 4000 nits.

\section{The Look}

The visual style or Look of a production is integral to the audience’s experience of the media, be it live-action, animation or game. It evolves from pre-production through to delivery and can be modified anywhere in the pipeline. In live-action, the Look is often adjusted, finessed and approved in the DI grade, which is the final stage of post-production. In animation and games, the overall process is very similar except that the finished Look is established earlier on during content development and a final DI grade is less common or less impactful to the Look.

The Look is the product of design, lighting, and grading. Crafting the Look involves many collaborators, many ideas and many processes. In order to view camera and CG scene-referred content in a meaningful way, it is important to add an approximation of the Look and an output transform that matches the display in use. Look Management is the process of accurately communicating the intended Look throughout production.

The existence of a look should always be considered even if it is not explicitly defined in a color pipeline. The earlier the Look is established the more decisions on lighting, VFX and editing are based on a common understanding. Even without making a choice, the project still has a look. 

\subsection{Establishing the Look}

Design influences in pre-production begin with the producers and director discussing ideas. The Look begins with these ideas and concepts, sometimes collected in the form of a design scrapbook. In live-action, the Directory of Photography (DP), costume and set designers, makeup, the post-production supervisor, concept artists, and others then work together to develop an early version of the Look, which is then built into a look transform as a LUT, ASC Color Decision List (CDL) or other similar proprietary formats. These early discussions drive the selection of cameras, lighting, stages, locations, costumes and image processing. Sometimes extensive camera and DI grading tests are made to confirm viability or assist in the decision-making process, but usually, ideas remain relatively fluid in the early period. The director's scrapbook often helps to communicate the influences as well as the intent.








A selection of final images from live-action visual effects, animation and games. The look of each is driven by the creative direction of the production. In this small selection, you can see a variety of choices around contrast, saturation and dominant colors.
Images are © 2018 MARVEL, © Disney/Pixar,  © Disney 2018 and © and ™ Lucasfilm Ltd. All Rights Reserved. Imagery from Battlefield V courtesy of Electronic Arts Inc, © 2018 Electronic Arts Inc. All rights reserved. Images © Disney/Pixar © Geoff Boyle All rights reserved.

If the Look evolves progressively from an established concept, through Look Management, and into the DI grade it results in higher quality deliverables and can also save a great deal of time, effort and money. Even with the most careful Look Management, there are likely to be many modifications in the final stage, the DI grade. More of this is discussed later in the 3.6 DI Grading. Sometimes the colorist may be asked to dramatically diverge from the original artistic intent, and this may be for any number of reasons. 





A shot from “Avengers: Infinity War” with a generic Rec. 709 look (top) and the show’s actual look (bottom)
© 2018 MARVEL
Once source images exist it is up to Look Management to enable all the contributors to view the images as intended, then for DI grading to perfect the Look on a scene by scene basis and to manage elements within each frame if necessary. The DI colorist is also responsible for managing any differences that occur as a result of different color gamut, dynamic range or resolution in the final delivery versions. There are two schools of thought on this. Some studios prefer that all versions are as similar as possible and are prepared to surrender the extremes of gamut and dynamic range to ensure that. Others seek to exploit the full use of high-dynamic range and wide color gamuts even though the differences between delivery versions are more apparent.

Crafting the Look in the widest color space, highest dynamic range and highest resolution provides a more future proof master deliverable, especially if the project remains scene-referred. Some organisations use this as a marketing differentiator. Lighting, makeup and production design decisions for an HDR screen master are very different to those made for an SDR television master, so it is best to consider the primary target display technology as early as possible.



A shot from “Rogue One: A Star Wars Story” with a generic Rec. 709 look (top) and the show’s actual look (bottom). The source colors for the bright magenta / pink of the X-Wing’s engines were outside of the Rec. 709 gamut.
© and ™ Lucasfilm Ltd. All Rights Reserved
In some productions, particularly in animation, the Look is set in early pre-production, with a little latitude for DI grading allowed to help continuity and atmosphere. There are situations when the Look may be quite different for some deliverables, e.g. for HDR Displays, or the Look may evolve over time, for example, TV series can look different between early seasons. These may need to be better matched for box sets later on. Often, the Look is not fully defined until the DI grading stage and then the extra flexibility of source referred VFX elements is advantageous. The Look can change even if the final DI colorist is involved from the very start either on-set or for the dailies. Changes to contrast, color or brightness might behave differently on CG material compared to camera sourced media. Whilst it is the intent in an ideal world that CG elements exactly match the scene-referred pixel values that the live-action camera would have captured, in reality, the capture process itself may have variations. Then the colorist has to correct for continuity as well as apply the Look. Differences between elements are also likely to occur if the Look Management is not applied correctly across all departments and facilities.



 A live-action image from a camera test shoot with an ACES Rec. 709 output transform (top) and a test grade crafted to even out the skin tones (bottom). 
Images are © Geoff Boyle
The responsibility of approving the Look can fall to the producer, director, cinematographer, colorist, VFX supervisor. Sometimes even the editor is involved. Often there are disagreements on the Look and the way it is interpreted scene-to-scene. For that reason, it helps to clarify whose vision the post-production team are working towards. Without clear direction, much time will be lost preparing alternate versions or re-rendering fixes in the final DI grade as decisions change. Whoever makes the decisions on the Look, someone then has to take responsibility for it through to delivery. That is usually either the Director of Photography or the colorist. The senior colorist is likely to be more accessible but may not be appointed until much later in the post-production timetable. Whilst there is a strong tendency for everyone to become familiar and comfortable with the Look set in the color managed pipeline it can happen that quite significant changes are made at the final grading stages in order to better communicate an impression to the audience.  Often the full resolution images are not seen on a large calibrated display in the edited sequence until the DI master grade when there is no longer an obligation to use reversible transforms. The DI colorist  can then finesse each scene to work within the Look or tweak the Look to better convey the artistic intent.

The Look for animation is guided by artwork created in visual development. Translating the creative style into a Look that can be applied to CG animation is an iterative process utilizing render tests of neutrally lit character rigs as well as more complete scenes that are representative of the storytelling environment. In a facility where there is a history of previous Looks, one of these may be selected as a starting point for further development. It can also help to start with a relatively neutral look before incorporating specific creative touches. A neutral look is useful for verifying the technical quality of renders and the accuracy of physical simulations.




A progression of Looks. (Top) A generic gamma 2.2 output. (Middle) Standard show Look with contrast and saturation boost, plus highlight compression. (Bottom) Final stylized Look incorporating orange-yellow filter. 
 Images are ©Disney. All Rights Reserved.

The initial pass at the Look tries to establish the desired contrast and overall shape of the tone curve along with the appropriate level of color saturation. Once this is established more specific adjustments can be made to dial in characteristics such as the behavior of bright highlights and shadow detail. A controlled amount of color distortion may be desirable sometimes, for example, to lend warmth to highlights. These fine adjustments may not be easy to make with a general test image, but can be made more obvious by selecting the right image content. Characters are particularly useful for fine-tuning the appearance of facial contours, highlights in the hair and the overall reproduction of skin tones, while environments are useful for evaluating color relationships and setting the behavior of very bright light sources. Because the final viewed color is a product of both the rendered or captured image and the Look it is viewed through, it is important to set the Look early so that changes do not impact approved images once production is underway. Once a base Look has been set for the movie, it may also be complemented by secondary Looks to support the story as it shifts into different environments.




A generic Rec. 709 look (top) and the project’s actual look for this scene (bottom). 
Imagery from Battlefield V courtesy of Electronic Arts Inc, © 2018 Electronic Arts Inc. All rights reserved.
The Look for a games project is established in much the same way as for an animated feature. Tests using environments and characters are run to establish the desired contrast, saturation and overall shape of any S-curve that may be required (if any). In the context of a game, a Look has to serve not just the vision of the art director but also the goals of the gameplay designers. If a Look is overly contrasty, it may interfere with gameplay by making enemies hiding in shadows effectively invisible to the player. By the same token, if the tone curve’s shoulder is too abrupt, the detail in the highlights may be lost, which can hinder gameplay if bright objects or enemies can’t be differentiated from bright skies or lights. Content is often validated against the game Look as well as a neutral Look. Since games are real-time and often built as a result of rapid iteration, the ability to change is held in high regard and thus content may be reused in multiple very different Looks. For example, gameplay may require a different Look for low health, or the game may be played at different times of day. Thus validating the content holds up in both the target Look and a neutral Look gives confidence the lighting or Look can be changed without requiring significant asset rework. Checking content in a neutral and project-specific Look mirrors animation and live-action workflows.

Games can be made with fairly small teams and as such, one person may perform many separate roles. There is generally no one person who tends to own Look creation, though the art director will typically drive it and make the final call to sign off on assets, lighting and Look adjustments made by a team of artists. One difference with live-action and animation is that because game engines generate new images every time the game is played, the opportunity to change and refine the Look extends even closer to the time when the audience sees the finished frames even tuning it to suit the individual display being used. By association, they have less ability to fall back to manually created and fine-tuned trim passes like visual effects and animation projects to optimize the display quality for different displays’ dynamic range and gamut. Although broad operating parameters can be set for the engine to work within and interpolate accordingly. Efforts such as the “HDR Gaming Interest Group” (HGIG) aim to optimize the images generated to best utilize the characteristics of each display.

Key Points
Even without making a choice, each project has its Look.
The Look influences audience perception of imagery.
The earlier the Look is established the more effective it is implemented.
Testing with a neutral look is useful for verifying technical quality and accuracy
The base Look may be complemented by secondary looks for some scenes


\subsubsection{Displaying scene-referred imagery}

One of the complexities associated with scene-referred imagery is display reproduction. While scene-referred imagery is natural for processing, displays can only reproduce a much lower dynamic (LDR) range. While at first glance it seems like a reasonable approach to directly map scene-referred linear directly to the display, at least for the overlapping portions of the contrast range, in practice this yields unpleasing results. See “Color Management for Digital Cinema” Giorgianni (2005), for further justification, and Section 3.2 for a visual example.

This challenge has been approached and intuitively solved by artists for millena. The world is a high dynamic range environment. Artists have striven to render artistic versions of reality on low dynamic  range media.  In the color community, there are many aspects to pleasingly reproducing high dynamic range pixels on a low dynamic range display, but one of the most important is known as tone mapping and is an active area of research. Surprisingly, many pleasing tonal renditions of high-dynamic range data to low dynamic range displays use similarly shaped transforms. On first glance this may be surprising, but when one sits down and designs a tone rendering transform there are convergent processes at work corresponding to what yields a pleasing image appearance. Other considerations for reproducing high dynamic range on low dynamic range displays include gamut mapping, highlight bleaching and saturation changes.

First, most tone renderings map a traditional scene grey exposure to a central value on the output display. Directly mapping the remaining scene-referred linear image to display results in an image with low apparent contrast as a consequence of the display surround. Thus, one adds a reconstruction slope greater than 1:1 to bump the mid-tone contrast.  Of course, with this increase, in contrast, the shadows and highlights are severely clipped, so a roll-off in contrast of lower than 1:1 is applied on both the high and low ends to allow for highlight and shadow detail to have smooth transitions. With this high contrast portion in the middle and low contrast portions at the extrema, the final curve resembles an “S” shape as shown below.

The ACES sRGB Output Transform applied to a ramp in the ACEScc color space.
An “S-shaped” curve is traditionally used to cinematically tone render scene-referred HDR colorimetry into an image suitable for output on a low dynamic range display. The input axis is an ACEScc logarithmic base 2 coding of scene-referred linear pixels. The output axis corresponds to code-values for an sRGB display.

The final transfer curve from scene-referred linear to display is shockingly consistent between technologies, with both digital and film imaging pipelines having roughly matched end to end transforms. If a film color process is characterized from negative to print, it almost exactly produces this “S-shaped” transfer curve, which is not surprising given the pleasant tonal reproductions film offers. In traditional film imaging process, the negative stock captures a wide dynamic range and the print stock imparts a very pleasing tone mapping for reproduction on limited dynamic range devices (the theatrical print “positive”). Broadly speaking, film negatives encode an HDR scene-referred image, and the print embodies an output-referred tone mapping. For those interested in further details on the high-dynamic range imaging processes implicit in the film development process, see the Ansel Adams Photography Series [Adams 84].

It should be noted that while there is significant convergence on filmic tone mapping, a filmic curve may not be desirable in games, tv or film. Productions choosing to reproduce a particularly stylized visual, such as a cartoony or highly ambient-lit appearance, may not wish to see the additional contrast or toe from such an operator, so may elect to have a less pronounced curve.

To summarize, it is strongly advised not to directly map high-dynamic range scene-referred data to the display. A tone rendering is required, and there is great historical precedence for using a global “S-Shaped” operator. HDR scene-acquisition accurately records a wide range of scene-luminance values, and with a proper tone mapping operator, much of this range can be visually preserved during reproduction on low-dynamic range devices.

Please refer to Section 3.2 for a visual comparison between an S-shaped tone mapping operator versus a simple gamma transform.

Specific considerations taken into account when crafting the Look for a film, animation or game are discussed later in 3.6 DI Grading

\subsection{Look Management}

Locking down the Look early on is a critical first step in crafting cinematic images, as it underpins every other part of the process. The remainder of the pipeline relies on visualizations that are created by working, translating or transcoding the Look and this is known as Look Management.

An overview of the filmmaking process shows that Look Management is core to each stage. The mistaken belief that the Look is the product of just the DP during shooting, or the colorist during mastering discourages a well-managed pipeline, without which the visual effects process is rather in the dark. Decisions earlier in the chain cannot easily be protected without a carefully color managed pipeline. Meaningful tests should be performed early on, saving a lot of work and some potentially unpleasant surprises later. In high-end productions changes to the Look can be suggested all through post-production and communicated to everyone else via color decision lists of some sort. The CDL is one such mechanism but is quite limited in scope, there are other more complex proprietary solutions in use.

When a look is applied to imagery as it is captured or saved to disk, it’s called “baking the Look”. The Look should not be baked in at the source or early in post-production. If the Look is baked in early, all the problems of output-referred workflows are very apparent, especially if the Look is strong. Continuity is more difficult and there is very little latitude to alter or modify the Look later on. Exposures, color balance, and lighting have to be close to perfect since there is little more information for the colorist to use in the final stages. VFX could still be delivered scene-referred, but the expectation is that they match the main content. Delivering to multiple formats is still possible, but usually, any differences that take advantage of wider tonal range or color gamut are severely limited. It is strongly recommended that the Look is NOT baked in until the final DI grading stage.
 
Working with multiple gamuts is a subject that can’t be avoided for a show of even moderate complexity. Almost every camera has a different encoding gamut. The working gamut used for a show will likely be distinct from any of the camera encoding gamuts. The working gamut will also likely be distinct from the final output display gamut. This is guaranteed if the show will be seen on multiple types of displays. The advent of the multi-gamut workflow has added an additional layer of complexity to setting up a project and communicating between artists, departments and companies. As such, it is especially important to define the choice of gamuts to be used for capture, grading, lighting, rendering, compositing, and final display as early as possible and to agree on how and when the gamut choices should be presented to users. For many artists, a successful setup will be one where they aren’t aware of the gamuts being used. OpenColorIO (2003)’s “role” feature allows some of these choices to be safely hidden from artists. It is very easy to get lost in a thicket of similar, but not quite matching, gamuts if images and color data aren’t properly tracked or labeled.
 
One of the benefits of ACES in the world of multiple gamuts is that it provides a clear standard for mapping between the different sets of gamuts used for cameras, rendering, compositing, grading, and final output.
 
The choice of working gamut affects the feel of controls and operations that affect color. Choosing a space that is too wide may break some operators, like keyers, or may just make otherwise familiar operators and controls feel unfamiliar to artists. Working in the camera encoding gamut may be useful for these jobs. For some operations, a compositor may transform to the camera encoding gamut, apply the operation and then transform back to the working gamut. This is similar in spirit to the time-honored trick of resizing in log described in 3.5.4 Working with Log Imagery.
 
When the reference Look is created in a DI grading system, the process is saved just as it is in case it needs to be modified or used to derive other looks. The Look itself is sampled in a 3D LUT format that can be loaded into color managed applications. Because some applications have specific requirements for the LUT formatting, it is not unusual to generate multiple LUTs. However, it is preferable to find a common LUT format that works with as many applications as possible with a high fidelity. Exporting the Look as a LUT is the easiest way to share it but using a parameter based method such as CDL is also an option for simpler looks. Many facilities use the combination of a show LUT with a CDL in front of it.

Once the Look has been established for a CG animation project the process that created it is saved just as it is in DI grading systems. The best solution is to use a color management system, like OpenColorIO, that is shared across all the applications and can apply Looks contextually. This allows for a high degree of automation in Look Management and can help eliminate operator error. Tagging images with metadata can assist this system by automatically managing looks across productions and also within a production when multiple looks are employed. A good color management system should be able to generate notes that can feedback to artists working at their desktops and also feed forward to the colorist for DI grading.

Games tend to be produced primarily using a single toolset, the “engine”. Within the engine itself, color management is largely handled automatically, being a single piece of software. For assets that are created externally to the engine, like textures and sometimes materials, it is desirable to have a representation of the Look of the project that can be used in the authoring packages used for those assets for validation purposes. In this case it is common to export a LUT or other representation of the Look, much like animation and live-action. Color management must also be handled with defined import/export paths allowing management of externally created content, ideally via similar tools to VFX/Animation but are sometimes handled manually.

Key Points
Look Management visualises the Look through production and post-production
Irreversibly applying the Look is called “baking in the Look”
The Look should not be baked in until the final DI grading stage
Look Management should consider multiple gamut sources and deliverables
Metadata is crucial for good Look Management

 
\section{On set}

An illustration of the recommended color managed approach for cinema or TV
*This LUT is generated from a CDL or proprietary saved grade format, which is then updated with any changes made in the dailies grade, and passed to DI as a start point


During filming, the aim is to capture the greatest amount of information possible, at the highest quality possible in order to give maximum flexibility in post-production, and to facilitate the integration of VFX elements. From a camera perspective, this means shooting in a raw format or log encoding and exposing to ensure that as much as possible of the dynamic range of the scene is captured. The way modern digital cameras apply the Exposure Index (EI) set while shooting is to alter the sensor level which maps to mid-grey, and in doing so a trade-off is created between the amount of latitude available above and below grey. The total dynamic range of a sensor is fixed. Rating a camera at a lower EI will mean lighting the scene more brightly for the same exposure, resulting in lower noise, which is beneficial for VFX. However, because doing this also reduces highlight latitude, care should be taken that important highlight detail is not clipped. In a studio, where the lighting can be controlled, this is not normally an issue, but on location, it is important to be aware of this limitation.

The monitoring output from the camera is the first opportunity for the production to get a preview of the intended look of the motion picture. When a look has been designed during pre-production, this can be applied to the signal sent to on-set monitors, complementing that part of the aesthetic of the film which comes from production design and costume. The look of a film can easily become ingrained in people’s minds based on how they first see it, so taking the opportunity to present the image in at least an approximation of its intended form at the earliest possible stage can be very beneficial. However, care should be taken if using extreme looks that the appearance of the image transformed through them does not negatively impact exposure decisions. It is advisable to also check the image through a standard neutral display transform, to ensure that a well exposed image is being recorded, clear of both the sensor noise floor and highlight clipping, and to allow for the possibility of different decisions about the look being made in DI grading.



\subsection{Camera profiles}

Color management should be used at every stage of the pipeline, and for material acquired with a digital camera, this can begin on set. All current digital cinema cameras capture a wide dynamic range and color gamut, and their encoding primaries and transfer functions are published by the manufacturers. Whether the recording is being made in a raw or camera log format, a log signal is usually available from a monitoring output of the camera. These are high-dynamic range, scene-referred representations of the image and can be used to apply a viewing transform, with or without a look included, which can be replicated and refined as the image passes through the various stages of post-production.

  
As can be seen from the above plots of some common camera log curves, the basic shape of all the curves is quite similar, with each having a straight line for a large part of the curve with a toe where it is compressed towards black. The value for lens cap black is often based on the 10 bit Cineon code value of 95 for Dmin. The straight line segment, in a plot with a logarithmic x-axis such as this, shows that the encoding is truly logarithmic in that part of the range, with an equal number of code values per stop. Its slope varies between encodings, as does the exposure represented by a log value of 1.0. It is therefore important to know what the encoding curve used for a given log image is, in order to be able to linearize it correctly. A LUT or ACES Input Transform designed for one camera may give a superficially pleasing result with an image from a different camera  but could cause unwanted clipping or distortion which is not obvious on all images. Using an incorrect LUT or Input Transform for aesthetic purposes is a risky strategy, which can cause problems further down the line. It is strongly recommended to use a correct linearization, and a separate operation for the look. It is important to understand that although a camera may use an encoding curve which has a particular dynamic range, this should not be taken to suggest that the camera itself can capture that range. The Sony FS7 for example, at default EI, clips six stops above mid-grey which maps to an S-Log3 value of 0.866. Values above this will not occur, even though S-Log3 can encode 7.74 stops above grey. Details of the encoding and decoding equations for various log curves is given in Appendix 4.1.

The cameral log encoding of the ‘A’ Camera on a production is often chosen as the working format for the final DI, since log encodings lend themselves well to the kinds of adjustments made during grading. They are constrained to the 0-1 range, or integer encoding of that range, and map the dynamic range of the scene evenly across the code values. Images from other types of camera can be transformed into the ‘A’ Camera’s log encoding. Alternatively, all cameras can be transformed into a non-camera-specific log working space such as ACEScct for grading. Ideally, this choice should be made in advance in consultation with the DI colorist, so any grading adjustments use the same working space.
    

Plots of the primaries of some common camera encodings

The above diagrams show the encoding primaries used by various digital cinema cameras, as well as those of ACES2065-1 and ACEScg. As can be seen, the CIE xy coordinates of the primaries vary considerably, so it is clear that a transform or LUT designed for one, will not work as intended on another. It is important also to be aware that these are simply encoding primaries, to which the raw sensor output of a camera is transformed. While they define the gamut of the encoding color space, this does not mean that the camera system in question can ‘see’ every color within that gamut. Indeed, since some primaries have negative coordinates, the encoding can represent colors which have no meaning in CIExy encoding. Despite this, a camera may produce pixels which appear to have these colors, particularly when lights with discontinuous spectra are visible in the frame. This is due to the residual error in the matrices chosen by the manufacturer to transform from the sensor’s raw RGB to the encoding color space. Cameras are not truly colorimetric, and so no 3×3 matrix can produce a perfect result. Manufacturers choose a matrix which minimizes errors in important colors such as skin tones. Care must be taken with images which contain these unreal colors, as the negative pixel values they contain may cause unwanted artifacts.

A more accurate matrix could be derived specific to the precise shooting setup, lighting, lenses, and filtration, but unless it is vital to have as photometrically accurate a result as possible, it is normally preferable to use the manufacturer provided matrix, such as those in the published ACES Input Transforms, to ensure matching processing by all vendors handling the media.


ARRI ALEXA, LogC, ALEXA Wide Gamut	ACES Rec. 709 Output Transform (pushed 0.5 stops)

Canon C500, C-Log3, Cinema Gamut	ACES Rec. 709 Output Transform (pushed 0.5 stops)

RED Dragon, Log3G10, RED Wide Gamut	ACES Rec. 709 Output Transform (pushed 0.5 stops)

Panasonic Varicam PL, V-Log, V-Gamut	ACES Rec. 709 Output Transform (pushed 0.5 stops)

Sony F-65, S-Log3, S-Gamut3.Cine	ACES Rec. 709 Output Transform (pushed 0.5 stops)
Images courtesy of Geoff Boyle, cinematography.net

The images above show how footage from different cameras can be matched by bringing them into a common working space and then applying the same display transform. The camera log images have been normalized by applying gain in scene-referred linear to bring the white chip of the chart to RGB = [0.9, 0.9, 0.9]. The difference between the log encoding curves is apparent as the differences in percieved contrast between the various log images, even though they encode the same scene luminance. The difference in the location of the encoding primaries manifests as variation in the apparent saturation of the log images, noticeable particularly in the green part of the chart from the RED camera. These differences in the way the same scene colorimetry is encoded are removed by using the appropriate Input Transform to convert to the ACES2065-1 colour space. The display transform is then applied by adding a half stop push for aesthetic reasons, and then using the ACES Rec. 709 Output Transform. It can be seen that even with the manufacturers’ published Input Transforms, there is still variation, particularly in the rendering of skin tones. The difference in the spectral responses of the different sensors (in fact primarily the differences between the spectral characteristics of the Bayer color filters on the sensors as well as the IR filtration) and the fact that some artistic choice is involved in characterizing a sensor, mean that each camera has slightly different feel to its image; this is unrelated to the differences between the appearance of the log encodings. ACES Input Transforms will not make all cameras look identical, but should provide a reasonable match up within the limits of each camera.
XXX
Key Points
Cameras from different manufacturers use different log encoding curves and primaries to represent the same scene colorimetry
These are just encoding methods, and if properly interpreted have no impact on the final look of the image
Cameras still have varying characteristics which do affect the look of the image
ACES Input Transforms bring different sources into a common working space so that the same Output Transform can be used for all sources, giving a close match of appearance

\subsection{Monitoring on set}

Most of the monitors used on set are likely to be provided by the video department and, since they are intended for checking of performance, framing and focus, may well not be accurately color calibrated. In order to maintain consistent color, at least one monitor should be properly calibrated, and in a more suitable environment (as specified by ITU-R BT.2035) for critical viewing of images, such as a blacked out tent. This is usually the responsibility of the Digital Imaging Technician (DIT).

The image displayed on this monitor is derived from the log feed from the camera, allowing the DIT to examine the unmodified scene-referred image using a waveform, false color, and other tools, in order to check exposure and shadow and highlight latitude. The image can then be viewed through the desired display transform, applied either by a LUT box or internal processing in the monitor.

The log output from the camera is likely to be a 10 bit SDI signal (often mirroring the floating point log encoding used in the DI) and is therefore well suited to the application of look transforms to preview the intended appearance of the film. However, it may well not represent the image at the full precision being recorded. Therefore, rather than being baked into the image, looks are normally represented in metadata form, which can travel with the rushes to be applied as appropriate later. The transform applied may be one universal Show LUT, per scene LUTs, the default LUT for that camera, or the image may be graded live by the DIT using dedicated software.

The ACES system defines an XML sidecar file, called ACESclip, which carries various metadata, including Look Transforms. At the moment this is not widely supported by post-production software, so while it may well become the de-facto approach in future, other workflows are normally used at this time. This can consist of proprietary saved grade formats or project files from DI software, or CDL may be used for greater compatibility at the expense of sophistication. See Appendix 4.5 for a description of the CDL format. LUTs are usually also created for baking into dailies and other look preview purposes.

A dailies colorist may then adjust the transform on a per shot basis to balance the image. Whatever transform is baked into the dailies and editorial, transcodes must then be tracked through post-production, so that VFX artists can see the same look when working on a shot, and bake it into renders for editorial, thus matching the original plate. If the colorist updates the grade, in advance of the final color, the transforms used in VFX should be updated to match this.

Something to be aware of when dealing with log encodings is that although, for example, both LogC and S-Log3 code black as ~0.093, the output of an ALEXA and an F55 with the lens cap on will be different. The ALEXA output will be at 9.3% on a waveform, and that of the F55 will be at 3.5%. This is because Sony Define S-Log3 in relation to the unscaled SDI code values, whereas ARRI define LogC in relation to ‘IRE’, represented by the 64-940 SMPTE range over SDI. See Appendix 4.7 for more detail on these Full and Legal ranges. This needs to be taken into account when building LUTs for use on set, as does the processing of the LUT box used, which may apply the LUT to the unscaled SDI code values, or may map 64-940 to float 0-1 before applying the LUT, and then back to 64-940 afterward.

While wide gamut and HDR monitors are available for use on set, it is still most common for the DIT’s monitor to be calibrated to the BT.1886 / Rec. 709 standard. Since there is a great deal of creative choice available in an HDR grade, and indeed variation in HDR view transforms, viewing the image with a generic HDR output transform will not necessarily be representative of the final HDR deliverable. Although the camera may be capturing a wide color gamut, the vast majority of colors found in photographic images are contained within the Rec. 709 gamut, so using this as a baseline output transform is normally considered adequate. Where HDR monitoring is used on set, an SDR version should always be referred to as well.

Key Points
The log output from a camera can be used to apply a look using the same approaches as will be used in the DI
Looks exist in the monitoring path only, and are communicated as metadata
ASC CDL is the lowest common denominator for communicating a look, as it operates identically in all systems


\subsection{Editorial}

Although modern Non-Linear Editors (NLEs) are capable of applying LUTs and other display transforms in real-time during playback, there is always a system overhead for doing this. In editorial, the ability to scrub quickly through the rushes is crucial, and anything which slows this process down can be problematic. Therefore the usual practice is to create transcodes in the native format of the NLE (either Avid DNxHD or Apple ProRes) with the look and output transform baked in. This is done either on or near set by the DIT or by a dedicated dailies colorist in a lab.

Metadata for the rushes is passed to editorial in the form of Avid ALEs or database files, and contains information about the display transform applied, as well as CDL grades and other information from set. This can include notes from the script supervisor, and information on lenses and filters used. Accurate tracking of this metadata through editorial is vital to ensure correct handling of the material further down the line. For VFX shots, a representative from the vendor is often present on set, and their notes can be cross-checked later with the metadata, to ensure the choice of correct lens grids and so forth.

Key Points
Baking the look into dailies for editorial is current best practice for practical reasons
Tracking look and other metadata through editorial is vital

\subsection{Digital Imaging Technician (DIT)}

It is the Digital Imaging Technician’s (DIT) responsibility to liaise between set and post-production. They need to communicate with those who will be responsible for dealing with the image further down the line, as well as with the cinematographer, to ensure they have an understanding of the intended color pipeline and required deliverables. The role originated as the person responsible for understanding the controls in video cameras, as productions moved to HD video from film acquisition so that the cinematographer could focus on lighting and exposing as they had done in the past.

Historically live grades were often applied to an output-referred signal from the camera, with a base viewing transform already baked in. The same grade applied before or after a display transform will give a very different result. This limits the look modifications that the DIT can apply to crude approximations of the intended look, useful only for visual reference rather than as reusable metadata. This is comparable to the way cinematographers have in the past experimented with looks in Photoshop. Now tools are becoming available which allow the cinematographer to craft a look with scene-referred images, using the same tools as will be used by the DIT on set, and later in the DI. The DIT should work with the cinematographer to ensure that this process is understood, and works smoothly.

If a live grading system is used, the DIT is responsible for creating primary grades to improve consistency from shot to shot. They will often maintain a library of stills from previous setups or locations, to aid the cinematographer in matching the lighting, and to even out the differences still further using simple color corrections. It is important if grades created on set are to be successfully passed on and used later, that the working space in which the color operations are applied, and the view transform used, is agreed beforehand and communicated clearly so that the look can be replicated accurately. Even within the standardized ACES pipeline, there is the option to grade in ACEScc or ACEScct. The result of using either of these is identical in the mid-tones and highlights of the image, but the look of the shadows will be different.

Key Points
A DIT must communicate with the camera department and post, and liaise between the two
For live grading to be truly useful it must use the same color management approach as will be used further down the line

\subsection{Reference Capture}


Typical standalone color checker rig for use in the VFX industry. The image has been white-balanced using gain adjustment on the Neutral 5 (.70 D) swatch, i.e. third swatch from the right of the bottom row, so that its value matches the reference sRGB luminance of 0.192. Note that while this is slightly higher than the 18% of a grey card, it usually does not matter in practical applications, and is commonly set to 0.18. If fine precision is required, it is important to keep that difference in mind. Also note that the grey ball in this particular image exhibits excessive glossiness from years of usage and should be recoated.

Along with the main camera image, it is important for VFX shots to capture additional reference, data, and metadata about the scene being photographed. Reference images of the set or props should be paired with images of other references such as a Macbeth Chart and an 18% grey card. Ideally, the reference images and reference objects would be captured using the main shooting camera and a DSLR that can store RAW images. If possible, it is best to use conversion software such as DCRaw or OpenImageIO to produce accurate scene-referred data. The Macbeth chart and 18% grey card can be used to balance the exposure of the reference and to remove the gross effects of the lighting environment if the reference will be used for texturing. If the reference capture is to be used for texturing specifically, it is best to shoot objects in a separate, as diffused as possible lighting environment. Professional photogrammetry services use white rooms with lights bounced off the walls or through diffusers, to achieve this effect.

Depending on the complexity of the VFX work envisaged, it may be necessary to take LIDAR scans of the set or HDRI environment captures. Chrome and grey spheres are often shot as quick lighting reference but should not be relied upon beyond that. The color handling of the LIDAR scanner or HDR capture device should be characterized before being used on set so as to understand the limitations of the devices. Most LIDAR scanners don’t capture color at either sufficient resolution or bit depth, so the result shouldn’t be used for anything beyond alignment. Many turnkey HDR capture devices, for example, offer only limited specification or control over color gamut. They mainly focus on the intensity range that can be captured. In this case, it is essential to shoot a Macbeth Chart with the main shooting camera, a DSLR used for reference and the HDR capture camera.

Lens distortion grids should be shot with all lenses used on the production, and notes should be taken as to which lens is used on each shot. These will be used later to remove lens distortions and vignetting. 

For VFX shots it is inadvisable to simply rely on the camera sheets or script notes, as these can often conflict. Embedded metadata in the captured images can be invaluable in helping verify notes and report data.

\subsection{HDR Environment Capture}

An HDR environment map captured for ‘The Avengers: Infinity War”. Luminance ranges go from 0 to 17100 
Images are © 2018 MARVEL

Panoramic HDR captures are the standard approach to capture onset lighting. A new HDR panorama should be captured each time the lighting changes. The classic technique to capture HDR panoramas for use as environment maps, first pioneered by Gene Miller, was to photograph a chrome ball, as  Recent advances in hardware now allow for directly capturing the scene-illumination, either with a camera using multiple, bracketed exposures or with dedicated hardware. One extension to this methodology is to capture the scene from multiple locations or heights. Triangulating the location of the lights in the series of captures allows for placement of area lights in the 3D scene with plausible energy estimations. Placed area lights allow for physically plausible rendering of local illumination effects, which contrasts with traditional skydome rendering approaches which assume infinitely distant lights. See Duiker (2003) and Selan (2011) for additional discussion.

Care must be taken when calibrating HDR lighting captures to account for colorimetry, linearity, and white balance, or the resulting lighting data may not integrate well with the computer-generated environments. It is common to use ND filters to darken the exposures, capturing even more information for the brightest parts of the environment for a given shutter speed and aperture. Fisheye lens usually won’t work well with ND filters. Adding a diffuse sphere and other reference materials during scene capture is useful in validating the reconstruction of the lighting later on. For outdoor day-lit environments, reference photos of a diffuse sphere may be used to recover or validate the sun’s light intensity, as it is not captured properly by even the best cameras.

There are some best practices when choosing the exposure and white balance for a HDR lighting capture, but there is an element of subjectivity in the choice of approach. Establishing the exposure of an 18% grey card facing the direction that you would expect an actor or the subject in the environment to face is one valid approach. Another valid approach is to establish the exposure of the 18% grey card when facing each of the cardinal directions, then to use the average of those exposures as the exposure for the scene. The white balance is often established using similar approaches. An additional approach to establishing exposure and white balance is to leave a grey card or Macbeth chart on the ground in the scene and then to neutralize the grey card or grey chips from the chart. This has the advantage of integrating lighting from the entire upper hemisphere. Note that much like primary color correction, the grey card should be neutralized only using simple multiplies with scene-referred linear imagery. Using matrices, adds, saturation effects or other more complicated operations could compromise the linearity and dynamic range present in the panorama. See Lagarde (2016) for a detailed discussion of the techniques and choices involved in capturing high quality HDR panoramas.

\section{Visual Effects, Animation and Games}

\subsection{Visual Effects}


Captured camera log plate (top), ACEScg workign space image (middle) and Rec. 709 final image (bottom). Shot form “Avengers: Infinity War”
© 2018 MARVEL
The traditional visual effects color pipeline is based around the core principle of doing no harm to the plate, to allow for the seamless intercutting of VFX and non-VFX shots. Plate photography is brought into the pipeline in whatever color space is delivered, typically the camera log, and then converted to scene-referred linear values using an invertible transform. Sometimes the camera’s encoding gamut is kept and only a pure 1D camera to linear transform is used. With a more modern approach, the linearization is accompanied by a matrix transform to convert to a wide-gamut working space like ACEScg. A working space gamut separate from the camera encoding gamut is especially useful if multiple different cameras, each with its own encoding gamut, are used during shooting. For visualization, a 3D LUT is used which emulates the eventual output transform. This was traditionally based on a print film emulation or a similar “S-shaped” curve, but is now more commonly created entirely in the grading suite. During the visual effects process, the output transform is never baked into the imagery except for intermediate deliveries such as editorial output or rough preview screenings. These deliveries are used to enable other parts of production, but are not the “final” deliverable. The delivery to the colorist should be the highest-fidelity representation of the original photography. Getting to this highest-fidelity representation typically involves going back to the camera encoding gamut and camera log space used when delivering plates, or rendering scene-referred linear to EXR files in a specified gamut.

\subsection{Animation}




A series of the elements used for a shot from “The Incredibles 2”. The three images represent the linear, Rec. 709 working space image (top) and the default look for a Rec. 709 display (middle) and the final show look for a Rec. 709 display.
 Images © Disney/Pixar
In animated features, all imagery is generated synthetically. Plate invertibility is a non-issue as there are no camera plates to match. The lack of real-world elements to match typically means that far more more latitude is allowed in the selection of viewing transforms. Despite such freedoms, it is often preferable to carry over those aspects of the traditional film workflow which are beneficial even in a fully-CG context. Think of an animated feature as creating a “virtual movie-set”, complete with a virtual camera, lighting and target display. Of course, while these virtual color transforms can be inspired by real-world counterparts, it is generally preferable to use idealizing simplifications which do not delve into the peculiarities inherent to color in physical imaging workflows.
Physically-inspired animated feature color pipelines require selection of a working-space gamut and a visualization, look or output, transform. The working-space gamut is chosen based on the deliverable needs of the project as well as authoring constraints of the production. If the show needs to deliver finals covering Rec. 2020, Rec. 709 and DCI-P3, it is important to choose a color space and authoring workflow that will cover these gamuts. The gamut of desktop monitors is a limiting factor on the creative utilization of wide gamut colors. Ideally, they should be matched to the primary wide gamut deliverable. Multiple manufacturers offer monitors that support P3, making this a viable target gamut for the desktop. Care should be taken that the reference monitor is calibrated to the correct white point and EOTF as well as primaries.
Like visual effects, the output transform for an animated feature was traditionally inspired by characteristics of film print stock but is now more commonly a product of the lighting supervisors, art directors and color scientists. It has diverged from film print stock characteristics due in part to the desire for bright saturated colors in animation, but also because digital cinema has become the primary market. It is now more common to match film prints to the digital cinema master than to match the digital cinema master to a film print. Like in visual effects, the output transform is only baked into intermediate deliveries such as editorial output or rough preview screenings. The delivery to the colorist should be the highest-fidelity representation of the rendered imagery. As floating-point images have gained more widespread support within software and hardware used outside of animation and systems like ACES become more widely used, it is now common to deliver floating-point EXR files directly from the CG pipeline to DI grading. Establishing a look for the output transform early in the animation process makes it the primary driver of the look of the final image. Grading for animations tends to have a lighter effect on the Look of the image than for visual effects, focusing on continuity and fine-tuning of the overall project.
Animated-feature color workflows have historically relied upon a single-step viewing transform, which directly converts from linear to the display, often using a gamma transform. However, physically inspired animated-feature color workflows (which separate the negative vs. print transform) are becoming increasingly preferred due to the ease of working with HDR rendering spaces and the robust hand-off to DI.
With the advent of multiple deliverables, often including HDR, working in a scene-referred space is particularly beneficial. The same scene-referred rendering can be passed through different display transforms, with only a trim pass in the final grade required to produce HDR and SDR deliverables.

\subsection{Games}




A series of the elements used for a shot from Battlefield V. The three images represent the linear, Rec. 709 working space image (top), the ungraded image with a generic Rec. 709 output transform (middle) and the final show look for a Rec. 709 display. 
Imagery from Battlefield V courtesy of Electronic Arts Inc, © 2018 Electronic Arts Inc. All rights reserved.
Games and real-time projects are very much like animations in that every aspect of the scene is simulated in CG: the virtual lights, cameras, set and characters, without being constrained to match a live-action plate. Developments in color processing in real time rendering have largely aligned with those in the visual effects and animation space, finding that approaches that maintain physical plausibility by computing lighting and material reflectance in scene-referred linear wide-gamut spaces and that separate the lighting and material reflectance from the display pipeline provide for the most realistic results and the most flexibility in targeting many types of displays.

Games have vastly reduced frame processing time compared to non-real-time CG, due to their frames being entirely generated in real-time, i.e. 30, 60, 90 frames per second or higher. As such they rarely have the luxury of working natively with 16 bit floating point data for textures, and instead operate primarily using heavily compressed hardware-accelerated texture formats which trade quality for a dramatic reduction in memory footprint and increase in performance. See section 3.4.4.1 Textures in games for more on this topic.

Another important performance, quality tradeoff to note is related to runtime floating point formats. When alpha channels are not needed and performance and/or memory footprint are more important than precision, games sometimes prefer to use floating point formats smaller than 16 bit. Colloquially known as “small float” or “mini float” formats, these floating point formats have the same 5 bit exponent size as 16 bit floats. They thus have very similar positive range to 16 bit floats, but they have a reduced size mantissa so precision is reduced. Most importantly these formats also do not have a sign bit, so do not support negative numbers and cannot store out-of-gamut colors. See “Small floats” in Appendix 4.11.

Much like animation, physically-inspired games color pipelines require selection of a working-space gamut and a visualization, or output, transform. The working-space gamut is chosen based on the deliverable needs of the game as well as authoring constraints of the production pipeline. If a game will need to render on devices covering Rec. 2020, Rec. 709 and P3, it will be important to choose a gamut and authoring workflow that will cover these display gamuts. At the time of writing, the majority of games are produced and rendered in the Rec. 709 / sRGB gamut, primarily for reasons of convenience including use of legacy assets and performance. Although some games do natively render in a wider gamut and it is expected for games to rapidly follow Animation into native support for wider color gamuts.




When it comes to gamut handling there does not seem to be a standardised approach in games. Many different options exist, each with their own tradeoffs, and it is likely different games will take different approaches depending on how this best suits their situation. Several options are highlighted below

The engine runs natively in the Rec. 709 gamut and all color data, such as textures, movies, and dynamically animated colors, are provided in this same gamut. Output may be in a wider gamut, but unless post processing also operates in a wider gamut then no wide gamut colors will be available.
The engine runs natively in the Rec. 709 gamut and the majority of color data, such as textures, movies and dynamically animated colors, are provided in this same gamut. The renderer uses signed floating point formats internally and employs the “scRGB” gamut which uses the same color primaries as Rec. 709 but supports wider gamuts through the use of negative numbers. Selected color data may be authored in a wider gamut and encoded in scRGB, requiring use of signed floating point texture formats which usually carry memory and/or performance overheads. See 3.4.4.1 Textures in games for more detail. This option can be appealing if the vast majority of color data is not wide gamut, only a few selected wide gamut assets exist, the performance and memory overheads are acceptable, and no obvious rendering artefacts are visible. See 3.4.6.2 The Rendering Gamut Impact for more detail.
The engine runs natively in a wider gamut, say P3 or Rec. 2020, and the engine employs color management to propagate the gamut of all color assets to the renderer. The renderer dynamically transforms all color data, such as textures, movies and dynamically animated colors, into the native gamut of the renderer at the cost of performance overhead in the renderer. Usually only gamut expansion is supported, as gamut reduction is more complex and expensive. A side benefit of this runtime transformation is that range compression/expansion (scale/bias) can be applied for free, allowing some artefacts associated with compressed textures to be minimised. Small float formats are supported. This option can be appealing if games can accommodate the additional rendering cost and complexity of managing the gamut mapping in the renderer.
The engine runs natively in a wider gamut, say P3 or Rec. 2020, and the engine employs color management to transform the gamut of all color assets into the renderer’s native gamut offline as a preprocessing step. This comes at the cost of a potential loss of quality due to quantisation and texture compression. See 3.4.4.1 Textures in games for more detail. The renderer presumes all colors are in the working gamut. Small float formats are supported. This option can be appealing if games cannot afford additional runtime performance overhead or renderer complexity but can accommodate some quality loss associated with storing wide gamut textures in low bit depth compressed formats, or are willing to pay the additional cost to disable texture compression and/or use less compressed formats in the case that visual artefacts are seen.

These are just a few examples of how gamut can be handled; it is likely that many more options exist and are in use. In any of the cases where wide gamut is supported, the necessary gamut reduction must be undertaken if outputting to a narrow gamut display. What is important to note is the need for color management of the source assets, and that the fundamental principles still align with the other CG approaches in this document regardless of how the renderer handles them.

Games also have a more varied approach to output transforms. Filmic Tonemapping by Duiker (2006) is a widely used approach that started with LUTs based on film print stock measurements but then gave way to analytical approximations, Hable (2010), to the original curve that form their own family of curves. Many games projects use the Reinhard (2005) algorithm, Drago (2003) Adaptive Logarithmic mapping and still other more ad-hoc approaches have been developed to suit each game. In recent releases, Unreal and Unity have both adopted elements of the ACES working spaces and output transforms. Games engines typically combine more limited output transforms with extensive, dynamic color correction controls, typically including an ability to use 3D LUTs. 

As the range of displays widens to include larger variations in dynamic range and gamut, separating the working space gamut from the display gamut and the color correction from the output transform becomes increasingly important. One advantage games have over the other CG productions is the entirely real-time nature of their rendering. Games can dynamically adjust their display mapping to best suit the capability of the display and can rely less on fixed mastering levels like the current set of ACES Output Transforms. This can help achieve the best possible quality for each display, but also carries with it gameplay implications. It is important for both competitive and social gaming that gameplay-essential luminance ranges be visible on all displays, to ensure games remain playable and no gamer is at a disadvantage if they are playing on a less capable display. This dynamic display mapping requires knowledge of the display characteristics, which is not always easy to ascertain. 


\subsection{Texture Painting}





Theses color maps represent the color modulation of the diffuse, specular and reflection components of a surface shader and the beauty render.  
© 2018 MARVEL
Formerly a majority of 2D texture painting applications worked most intuitively in output-referred color spaces, where the painted image was directly displayed on the screen. However, the texture authoring landscape in the VFX industry has dramatically changed with the commercial release of Mari (2010) and other 3D paint packages like Substance Designer (2010) and Substance Painter (2014). These 3D painting packages adopt a scene-referred rendering workflow where artists are able to paint plausible reflectance values while reviewing their work in a viewport that approximates the shading and lighting of the final render and includes the in-house or client output transform. Because 3D viewports are driven by physically-based real-time renderers that strive to present a close approximation of the asset appearance as computed by the offline renderers, artists are able to paint much more in context.

 Examples of the effect of varying material parameters. Each parameter is varied across the row from zero to one with the other parameters held constant. - Physically-Based Shading at Disney, Burley, 2012

One point worth discussing is the somewhat muddied question of whether a texture is scene-referred or output-referred, or if scene-referred, what that means. For diffuse textures, it is relatively easy to say what it represents at any point in the lighting and rendering pipeline: diffuse reflectance values specified in the working gamut used for asset development. For textures that represent specular roughness, normal distribution, sub-surface scattering mean free path or other more technical aspects of a surface’s response to lighting, the texture may represent a linear value, a gamma-encoded value, a log value or values from another bespoke measurement space. The connection between these values and the working gamut for asset development is also less clear. The values in those textures are best thought of as being “parameter-referred” as they only have meaning for the parameterization of the surface shader being used. The choice of which parameters to expose for a given material and which measures and space to use for them is a subject of much debate. See Burley (2012) for an interesting discussion of the considerations involved in parameterizing surface shaders. Given the dependence on the surface shader parameterization, painting textures in a 3D paint package with a physically plausible shader that shares the parameterization of the production renderer becomes much more important. Painting textures in a 2D application for parameters that aren’t easily mapped to a display can be an exercise in frustration, often leading to a guessing game as to what effect a change to the texture will have on the look of a 3D asset. Regardless of the approach chosen, one must distinguish painted color maps and data maps: bump maps, normal maps, iso maps, control maps, etc, which should not be processed colorimetrically.






A shot from “Rogue One: A Star Wars Story” including an X Wing model (top) with the painted diffuse (bottom, left), specular intensity (bottom, middle) and specular roughness (bottom, right) maps.
Texture reference might be sourced from the internet or other sources of output-referred imagery such as the JPG/TIFF output from a digital camera. In this scenario, a common approach is to utilize an inverse tone rendering, to convert output-referred imagery to a hypothetical scene-referred space. Conceptually, the texture artist is painting the tone rendered appearance of the texture, not the texture itself. There are a few challenges with this conversion from output-referred to scene-referred space. First, the tone rendering may not be invertible, particularly when filmic 3D LUTs are used for image preview. Second, traditional s-shaped tone renderings have very horizontal portions of the curve, which when inverted result in very steeply sloped transfer functions. This steep contrast has the effect of amplifying small changes in input code values. For example, a steep inverse could result in the situation where a single code value change in a painted texture could correspond to a delta of a stop or more of scene-referred linear light. This sensitivity is very challenging for artists to work with.

For text and logos, the goal is often to convert to scene-referred linear through the inverse of the project Look, specifically so that when the text or logos are integrated with other imagery and run through the forward version of the project Look, the original text and logo colors are returned. Thus, the overall effect is a no-op on the logo and text.

A common solution to these issues is to be a bit more modest in the goal of perfectly inverting the display transform. Simplifying the problem, we can instead aim to create an approximate 1D inverse, tuned to be well behaved both in terms of color performance and dynamic range. Of course, as a consequence of this simplification, the texture painting is not truly WYSIWYG. Thus, a residual difference visualization 3D LUT is crafted for accurate color preview in the texture painting tool.

Another axis of variation in texturing is when to perform the conversion to scene-referred linear. The suggested approach is to linearize prior to mipmap texture generation. The primary advantage of this approach is that scene-referred linear energy is preserved through all the mipmap levels, allowing for the highest fidelity sampling and fewest color space related artifacts. Further, disallowing color space conversions at shading time prevents shaders from using undesirable, non-linear, color math. The disadvantage is that the storage requirements for linearized data are potentially increased, i.e., even if a texture is painted at 8 bits of precision in an output-referred space, increased bit-depths are required after conversion to scene-referred linear. When dealing with 16 bit painted textures, this is less of a concern as the delta in file size is smaller.

It is common for facilities to have an Onset Capture department whose responsibility is to acquire the texture references for a show. They are often processed with a flavor of DCRaw, Libraw or a dedicated in-house tool that output the texture reference data as scene-referred linear light values encoded with the facility working color space inside an EXR file. In a controlled environment, the use of cross-polarizing filters and polarised lighting during acquisition further allows for increased reference texture fidelity as the specular response of the sample is attenuated producing a more useful representation of its reflectance. See 3.3.5 On-Set Reference Capture for more detail.

\subsubsection{Textures in games}
As mentioned in section 3.4.3, games are often biased heavily towards the performance end of the performance, quality tradeoff. Games make much use of heavily compressed hardware-accelerated texture formats which trade quality for a dramatic reduction in memory footprint and increase in performance.
Compressed textures are not required, but are often used. In addition, there is not always a one to one correlation between the storage format and the runtime format; some games transcode from one format to another to ensure minimal storage or loading overhead while still being able to leverage hardware texture compression in the renderer.
By far the most common formats are 8 bit and unsigned (storing positive-only values) although some are 16 bit. 16 bit formats are larger and often slower, so tend to be used sparingly but are still useful especially those that are floating point with support for a sign bit. It is usual for textures to be authored at high quality, but resized and compressed in a conditioning pipeline before they are stored on disk or streamed.
HDR formats are supported by hardware in various guises, including several variants of 16 bit floating point: uncompressed, small float formats with no sign bit, compressed with no sign bit and compressed with a sign bit, and compressed shared exponent formats like RGBE also with no sign bit.
Since texture compression is lossy and involves bit depth reduction, artefacts including quantisation can be seen. In this case it can be beneficial to scale/bias SDR textures to use the full 0-1 range before compression, propagate this scale/bias to the game engine and remove it at runtime. This can minimise artefacts for a small per-texture runtime cost. 
See “Block Compression” in Appendix 4.10 for more information on compressed textures.
Another GPU hardware feature used extensively by games is support for free OETF/EOTF application. Since scene-referred linear rendering is preferred, but textures are usually stored perceptually encoded, GPUs feature hardware to apply a free EOTF to each texel fetched, linearizing these values before filtering and before they are used as inputs to the renderer. In addition GPUs can apply a free OETF to a value before writing it to memory if this is required. Unfortunately, the only transfer function supported is the sRGB. See 4.1.3 sRGB for the definition of the sRGB transfer function. At time of writing there is no hardware support for any other transfer function, including PQ. Even with that support, it is important to note that only the transfer function part of the sRGB specification is provided; there is no gamut handling in hardware.

\subsection{Matte Painting}

Matte painters and paint packages work in as many different ways as there are facilities and software packages. In this example, the painter works in an output-referred color space. As renders are natively in scene-referred linear, a mapping from the matte painting space to the rendering working space is required, regardless of which approach is chosen.
It is often useful to consider matte painting and texture painting as two different color workflows. In matte painting, it is very common for the artist to want to paint values which will eventually utilize the full scene-referred linear dynamic range, even when working in an output-referred space. Thus, an inversion which allows for the creation of bright specular values is preferable. In such cases, it is often convenient to provide a visual preview both at a normal exposure, as well as a few stops darker, to let the artist have a good feel for the dynamic range in their plate. Applying an inversion in this context has many of the potential downsides described for texture painting, namely that an exact inverse isn’t guaranteed to exist for all color and that the inverse of a tone curve with strong highlight compression may lead to excessively bright values when an output-referred image is brought back into the working space.
To avoid those issues, some workflows elect to perform matte painting on scene-referred values. The scene-referred values are typically encoded in a non-linear fashion for compatibility with integer based tools and to utilize the largest dynamic range possible. One option is to use a logarithmic encoding and to provide a viewing transform that incorporates the look for the show. This will be familiar to artists who have experience with films scans, but it also changes the feel of paint tools that are normally used in a display-referred context. Another option is to use a hybrid log-gamma encoding. This hybrid encoding uses a curve which is gamma-shaped for the standard range of image values and transitions into a logarithmic shape for bright values like highlights. The gamma portion of the curve allows tools that are built for a display-referred working space to continue to work as expected, including blending operations which are designed for gamma-encoded material. The logarithmic shape for highlights allows details to be preserved throughout the process. Artists can paint directly on the matte image without any additional viewing transform. A viewing transform that incorporates the look for the show is still useful, to see how the painted values will appear in their final presentation. These viewing transform can be applied using an ICC profile or a 3D LUT as supported by the paint application.

\subsection{Lighting, Shading, and Rendering}

The stages of rendering, lighting, and shading most closely approximate physical realism when performed in a color space that is scene-referred linear, high-dynamic range and wide-gamut. Ideally, no color space conversions should be required during the rendering process, as all input assets such as textures, plate re-projections, and skydomes, can be linearized and gamut mapped beforehand. 

Image viewing of scene-referred linear data is typically handled by converting to the color space being delivered to digital intermediate, often a log encoding of the color space, and then applying the view transform suitable for the specified display. For convenience, these transforms are typically baked into a single 3D LUT, though care must be taken to assure the LUT has suitable fidelity over an appropriate HDR domain. A simple cube 3D LUT is not suitable for use with linear data. A 1D shaper LUT, or mathematical transform needs to be applied prior to the cube to give the mesh points a more appropriate spacing. See Appendix 4.4 for more detail on LUTs.



A rendered image with no output transform applied (top), an image with a generic Rec. 709 output transform (middle) and the show’s final look (bottom).
Images from “Moana” Courtesy of Walt Disney Animation Studios. Images © Disney. All rights reserved.

In this raw visualization of a high-dynamic range, scene-referred linear render, values from the source image greater than 1.0 are clipped when sent to the display. Using a naive gamma 2.2 visualization to map scene-referred linear to output-referred values results in an image with low apparent contrast and clipped highlights. Observe clipping in the wave and eye. Using an S-shaped tone curve applied in a log space to visualize the scene-referred linear render yields a pleasing appearance of contrast, with well-balanced highlight and shadow details.

\subsubsection{Scene-referred linear is preferred for lighting}

A render is a simulation of a real scene. In the real world, there is a linear relationship between the incoming and outgoing energy from a surface, i.e. the incoming light that is scattered or absorbed. To simulate that interaction properly, working with units that refer to the physical units of energy in the world and applying operations that maintain the linearity of these relationships makes the simulation more straightforward. 

First, the render itself benefits. Physically plausible light transport renderer mechanisms such as global illumination yield natural results when given scenes with linear high-dynamic range data. Physically-based specular models combined with area lights produce physically plausible results with high-dynamic range data. Rendering in scene-referred linear also allows lights and material components to be re-balanced post-render, with results that track identically as if the original render had been tweaked. Storing rendered images in a floating-point container is most common to preserve maximum image fidelity; the EXR format is most widely used in VFX and animation and increasingly in games and for interchange in post-production. See 4.9.2 EXR for a more in-depth description of OpenEXR.

Figure 1 from ‘OpenEXR Color Management’ by Kainz (2004), ILM advocating for the use of scene-referred linear values in lighting, rendering and compositing. Many of the devices and acronyms have changed, but the core concepts remain valid.

As with preserving the dynamic range of the camera into DI Grading, rendering CGI in a high-dynamic range scene-referred linear space makes it simpler to produce HDR and SDR versions of the film. Rather than baking in tone and gamut mapping applicable only to one display format, different output transforms can be applied to the same scene-referred image data to view it on HDR or SDR displays.
One issue to watch out for with high-dynamic range renders is render noise. When scene textures like skydomes contain very bright areas, like the sun or other compact light sources, i.e. areas that substantially contribute to the scene illumination with relative luminance values well above 1.0, care must be taken in sampling to avoid noise. Rendering techniques such as multi-importance sampling (MIS) are useful to mitigate such issues. Even still, it is common to paint out very compact or very bright light sources, such as the sun, from skydomes, and then to add them back into the scene as native renderer lights to allow for both lower-noise sampling and often greater artistic control.

Light shaders also benefit from working with scene-referred linear, specifically in the area of light falloff. In the distant past, the default mode of working with lights in computer-graphics was to not use falloff. However, when combined with physically-based shading models, using an inverse-square law light falloff behaves naturally as the relative size of a light diminishes with the square of the distance as a light recedes from a surface, or vice versa. Note that this is a simplification of the full solid angle-based equation for the projected area of a surface or light and that this can be problematic in particular when scenes are modeled with large units, say 1 unit is 1 meter, and it becomes common for objects to be less than one unit from lights. In this case, the distance value less than 1 drives the results of the inverse-square law falloff term to be considerably greater than 1, no longer resembling a falloff. If one tries to shoehorn realistic lighting falloff models into output-referred rendering, the non-linearity of output-referred spaces make it’s very difficult to avoid clipping and generally implausible results. On the downside, one consequence of working with natural light falloff is that it’s often required to have very high light intensity values. It is therefore common to express light intensity in user interfaces in terms of “exposure” or “stops,” as it’s much friendlier to the artist to present an interface value of ”+20 stops,” compared to an RGB value of “1048576.0”.

Anti-aliasing operations also benefit from using scene-referred linear values, though one must be more careful with renderer reconstruction filters. Negatively lobed filters such as Catmull-Rom have an increased tendency to exhibit ringing artifacts due to the extra dynamic range. This is a particular problem on elements lit with very bright “rim lights,” as this creates bright specular highlights directly in contact with edges. The core challenge here is the same as when handling very bright, very small sections of imagery in compositing. There are a number of common approaches to working around such filtering issues. First, for operations that shows ringing artifacts, switching the filter to a box or gaussian filter will remove the artifact, at the expense of softening the image slightly. If the issue only happens in a few places, this may be a reasonable approach. Second, very bright specular samples can be rolled-off such that these samples do not contribute such large amounts of energy. However, this throws out much of the visually significant appearance which adds so much to the realism. Another approach is that the extra energy can be spread amongst neighboring pixels such that the specular hits show an effect analogous to camera flare. Both of these effects can be implemented in compositing. 

\subsubsection{The Rendering Gamut Impact}

A shot from “Rogue One: A Star Wars Story” with a green beam of light that is far outside of the Rec. 709 gamut.

One decision that has taken on more prominence recently is the choice of gamut to use for rendering. It is important to acknowledge that the rendering gamut affects the way computations are performed, especially indirect lighting as shown by Agland (2014). Rendering an image using sRGB textures and then converting it to ACEScg will yield a different image than having rendered with ACEScg textures in the first place.

Mathematical operations are dependent on the chosen basis vectors. In the space of color, the basis vectors are defined by the choice of RGB color space primaries. The same operations performed in different RGB color spaces will yield different tristimulus values once converted back to CIE XYZ color space. For example multiplication, division and power operations are dependent on the RGB color space primaries while addition and subtraction are not. Quoting Rick Sayre from Pixar:

“The RGB basis vectors typically become non-orthogonal when transformed to XYZ, and definitely so in this case. Thus there should be no surprise that component-wise multiply does not yield a proper transform between two non-orthogonal spaces.”


Illustration of the effect of multiplying various colors by themselves into different RGB color spaces: the resulting colors are different. The various samples are generated as follows: 3 random sRGB color space values are picked and converted to the three studied RGB color spaces, they are exponentiated, converted back to sRGB color space, plotted in the CIE 1931 Chromaticity Diagram on the left and displayed as swatches on the right.
This introduces the concept that some RGB color spaces are more suitable than others when it comes to 3D content generation and computer graphics imagery in general. This problem is typically solved by using spectral rendering or leveraging a spectral renderer as the ground truth base against which the RGB color space is selected.

Renders of the same scene using Rec. 709 primaries (first row), 47 spectral bins (second row), Rec. 2020 primaries (third row), spectral minus Rec. 709 primaries render residuals (fourth row), spectral minus Rec. 2020 primaries render residuals (fifth row). The last row shows composite images assembled with three vertical stripes of respectively the Rec. 709 primaries, spectral and, Rec. 2020 primaries renders.

Tests and research conducted by Ward and Eydelberg-Vileshin (2002), Langlands and Mansencal (2014) and Mansencal (2014) showed that gamuts with primaries closest to the spectral locus, i.e. spectrally sharp primaries, tend to minimize the errors compared to spectral ground truth renders. 
Usage of gamuts such as Sharp RGB, Rec. 2020 or ACEScg is often beneficial for this reason but it is contextual and sometimes, Rec. 709 can perform slightly better. In the previous image, direct illumination tends to match between the renders. Areas that show the effect of multiple light bounces, i.e., the ceiling, in the Rec. 709 and Rec. 2020 primaries renders tend to exhibit increased saturation, especially in the Rec. 709 primaries render or slight loss of energy, especially in the Rec. 2020 render. Excluding outliers, e.g., the visible light source, the RMSE with the spectral render are 0.0083 and 0.0116 for respectively the Rec. 2020 primaries and Rec. 709 primaries renders.

As of the writing, these is no first principal or formal mathematical proof that one particular working space is best for computer graphics. As demonstrated, any choice of gamut will introduce error relative to ground truth spectral renders though wide gamut spaces such as Rec. 2020 or ACEScg tend to match spectral ground truth renders most closely. The working-space gamut is typically chosen based on the deliverable needs of the project as well as authoring constraints of the production. If the show needs to deliver finals covering Rec. 2020, Rec. 709 and DCI-P3, it is important to choose a color space and authoring workflow that will cover these gamuts. See 3.4.1 Visual Effects, 3.4.2 Animation and 3.4.3 Games for considerations specific to different types of projects.


\section{Compositing}




A shot from “Rogue One: A Star Wars Story”, starting with a CG background plate (top), adding in a live-action foreground element (middle) and the final composite image (bottom). All three images have an Output Transform for a Rec. 709 display. 
© and ™ Lucasfilm Ltd. All Rights Reserved

Compositing is the process where live-action and CG elements are manipulated and merged. Image processing in scene-referred linear and logarithmic encoding spaces are both useful, though scene-referred linear is the default. As in lighting, image display typically leverages viewing transforms that emulate the final output. Examples of commercially available compositing applications include Foundry Nuke, Adobe After Effects, Blackmagic Design Fusion, and Autodesk Flame.

In feature film visual effects, live-action photographic plates are typically represented on disk either as raw captures of the camera data, as a log encoding of the linear camera data, often as DPX files, or as linear EXR files. Log files are typically encoded using the encoding primaries of the source camera, such as ALEXA Wide Gamut, whereas EXR files may use camera primaries or generalized primaries such as the AP0 primaries of ACES2065-1.  When working with log frames, the compositing package typically converts the images to scene-referred linear on input using a manufacturer specified transfer function, and a matrix transform to the working-space gamut if required, and converts back to the original color space on output. In its simplest form, the end to end compositing process represents a no-op, aiming to leave pixels not modified by the effect exactly as they were in the original plate. Such behavior is absolutely critical, as typically not all the shots will go through the VFX workflow, and VFX processed shots must seamlessly intercut with the remainder of the motion picture.

\subsection{Scene-referred linear is preferred for compositing}

The benefits of scene-referred linear compositing are numerous; similar to the benefits found in rendering, shading, and lighting. All operations which blend energy with spatially neighboring pixels: motion blur, defocus, image distortion, and resizing, for example, have more physically plausible, aka realistic, results by default. Anti-aliasing works better. Light mixing preserves the appearance of the original renders. Most importantly, even simple compositing operations such as over produce more realistic results, particularly on semi-transparent elements like hair, volumetrics and fx elements.


San Francisco City Hall at Night - An HDR scene-referred linear image (top left), Defocus applied to LDR output-referred values (top right), The original scene defocused in camera (bottom left), Defocus applied to the HDR scene-referred linear data before the output transform (bottom right). All four images use the ACES sRGB Output Transform.
These image helps illustrate the energy implications of using different linearization philosophies. Pay particular attention to the appearance of the lights in the bell tower on the top of the dome. How much energy is represented in those pixels? The least realistic defocus effect is achieved by applying the filter kernel directly to the output-referred pixel values without any linearization. Since in this case there is only a small difference between the pixel values of the lights and that of diffuse white, the defocus causes the lights to be lost in the surrounding pixels. 


San Francisco City Hall at Night - The original scene defocused in camera (left), Defocus applied to the HDR scene-referred linear data before the output transform (middle). Defocus applied to pseudo-linearized output-referred data (right). All three images use the ACES sRGB Output Transform.
Applying the same defocus to image data which has been converted to an approximation of scene-referred linear using a simple gamma or sRGB curve is an improvement, but still tends to de-emphasize the specular highlights. This is because even though the operation is being applied in ‘linear’, it is downstream of the output transform, and the highlights have already been tone mapped for a lower dynamic range display, restricting them to the 0-1 range, and thus do not have physically-plausible amounts of energy. Applications like Adobe After Effects work in this way by default when the “use linear” option is chosen for filters, simply removing an assumed 2.2 gamma before the filter and reapplying it afterward. Applying a defocus in scene-referred linear reveals a bokeh effect on each of the bell tower lights, mimicking the visual look of having performed this defocus during camera acquisition. This is because the pixel values are proportional to light in the original scene, and thus very bright pixels have sufficient energy to remain visible when blended with their neighbors. Other energy effects such as motion-blur achieve similar improvements in realism from working in a scene-referred linear space.

\subsection{Scene-referred linear compositing challenges}

There are some challenges with working with high-dynamic ranges in compositing. First, filtering operators that use sharp, negative lobed kernels like as lanczos, keys, and sinc are very susceptible to ringing, i.e. negative values around highlights. While interpolatory filters such as box, gaussian, bilinear and bicubic cannot cause this artifact, they tend to introduce softening. When sharp filtering is required like lens distortions, resizing, and other spatial warping techniques a variety of approaches are useful to mitigate such HDR artifacts. A non-exhaustive set of methods are listed below

The simplest approach to mitigating overshoot/undershoot artifacts is to roll off the highlights, process the image, and then unroll the highlights back. While this does not preserve highlight energy as it has the net effect of reducing specular intensity, the results are visually pleasing. The approach is also suitable for processing images with alpha.
For images without alpha, converting to log, filtering, and converting back will greatly reduce the visual impact of overshoot and undershoot. Though log-space processing results in a gross distortion of energy, for conversions which don’t integrate large portions of the image, such as lens distortion effects for plates and not blurs, this works well.
For images without alpha, a similar method is to apply a simple invertible tonemap, filtering, and then inverting back. A method as simple as 1/(1+color) produces pleasing results by effectively weighting each sample as a function of its brightness.
Another approach to HDR filtering is to apply a simple camera flare model, where very bright highlights share their energy with neighboring pixels.

For all approaches that undertake a form of perceptual encoding, care must be taken to ensure enough precision is maintained during filtering. 16-bit floating point may not be suitable.


An image with negative black halos around highlights, produced by a Lanczos 6 resizing filter with negative lobes (left) and the image converted to a log space before resizing, thus avoiding negative black haloes (right).
Applying negative-lobed filter kernels to high-dynamic range images, may cause visually-significant ringing artifacts introducing black silhouettes around sharp specular highlights. Ringing artifacts may be avoided when using sharp filters on HDR imagery by processing in alternate color spaces, such as log, using energy-roll off approaches, or by pre-flaring highlight regions. This image demonstrates the results of the roll-off technique.

Another challenge in working with scene-referred linear data in compositing is that tricks often relied upon in integer compositing may not be effective when applied to floating-point imagery. For example, the screen operator, which remains effective when applied to matte passes, is sometimes used on RGB data to emulate a “partial add”. As the screen operator is ill-defined above 1.0, unexpected results will occur when applied to HDR data. Some compositing packages swap out screen for max when either input is outside of 0.0-1.0, but this is primarily to prevent artifacts and is not artistically helpful.  Alternative “partial add” maths such as hypotenuse are useful but do not exactly replicate the original intent of screen. Another related issue to beware of when compositing is that when combining HDR image, alphas must not go outside the range of 0 to 1. While it is entirely reasonable for the RGB channels to have any values, even negative in certain situations, compositing operators such as over, produce totally invalid results on alphas outside of the range 0 to 1.

One subtlety of working with floating point imagery is that artists must become familiar with some of the corner cases in floating-point representations: NaNs and Infs. For example, when dividing by very small values, such as during un-premultiplication, it is possible to drive a color value up high enough to generate infinity. NaNs are also frequent and may be introduced during lighting and shading inside the renderer, or during divide by zero operations. Both NaNs and Inf can cause issues in compositing if not detected and cleaned, as most image processing algorithms are not robust to their presence and generally fail in unexpected ways.

\subsection{Negative Values}



A frame from ‘The Avengers: Infinity War”. The frame in the original Alexa LogC encoding (top), The frame transformed to ACEScc with negative values in shown as purple fringing visible in particular on the top right light (middle) and the final frame (bottom)
© 2018 MARVEL 


CIE 1931 2° Standard Observer chromaticity plot of the image above
The above image contains pixel values which are positive in ALEXA Wide Gamut but negative in ACEScg as they are outside of the ACEScg gamut. This results in overly strong purple fringing and clamped values when viewed through an ACES Output Transform. The frame above the light shows the issue clearly. Even pixel values which are on or near the inside of the gamut boundary can result in artifacts. There are a number of potential approaches to solving this problem. Working in the original camera encoding gamut, ALEXA Wide Gamut in this case, is one option. Creating a technical grade for the shot which removes the artifact, and providing that as a LUT for use in VFX is another. With that option, it will be obvious if anything in compositing makes the artifact reappear.



Negative values may occur in a scene-referred linear color if the working gamut is smaller than, or even just different from, the encoding gamut of the plate. In some extreme examples, cameras have been found to produce values outside of their specified encoding gamut. That could lead to negative scene-referred linear values even without transforming away from the camera’s encoding gamut. Like NaNs and Inf values, this comes with the territory. Don’t just clip these values! These negative values can pass through some operations without causing issues and should normally be preserved when delivering VFX renders to DI. However, the results of some mathematical operations used in compositing are undefined for negative numbers and may create NaNs or Infs, or push a small negative value to a much larger one. Sometimes, where negative values will be clamped by the final display transform, it may be safe to clamp them in the comp. This is a risky strategy though. Changes to the grade may make the clamping visible, causing artifacts such as black holes in the image. It is generally preferable to use an approach such as offsetting the values before an operation to make everything positive and then subtracting the offset afterward. Remember that compositing should be a no-op for unaffected pixels, so even if a pixel value represents an unreal color in the plate, the VFX shot should be returned to DI with values representing that same unreal color, matching the original plate.

This is also something to watch out for in games. As mentioned in Section 3.4.3 Games, games may render in less precise “small” floating-point formats when an alpha channel is not needed and performance and memory bandwidth are more important than precision. It is important to remember that these floating point formats do not have a sign bit so do not support negative numbers, and thus cannot store out-of-gamut colors. If asked to do so, clipping will silently occur. Care must be taken to either choose the appropriate floating-point format, handle negative values explicitly or bring out-of-gamut values into the working gamut to avoid the silent artefacts that would otherwise occur.

\subsection{Working with Log Imagery}




Log plates captured for “Avengers: Infinity War” (left, right) and “The Amazing Spider-Man” (middle). 
Images from “Avengers: Infinity War” are © 2018 MARVEL.  Images from “The Amazing Spider-Man” Courtesy of Columbia Pictures. © 2012 Columbia Pictures Industries, Inc. All rights reserved.
Despite scene-referred linear having benefits in many compositing operators, log-like spaces are sometimes still useful in modern compositing workflows. This is in part due to it being very straightforward to express some operations in a log space, like exposure or contrast adjustment, and in part, because some packages were designed around processing log imagery so the controls and feel of the operations are more intuitive, or familiar to artists, with log images. Modern compositing applications can apply almost every operation directly to scene-referred linear data, but it may still be useful to work with log imagery. In log space, both the linear and gamma contrast operators have reasonable performance. This is one of the reasons that colorists often prefer to work in log-like spaces. Other operations which can be useful to perform in the log domain are grain matching, pulling keys, and many spatial distortion operators. But care must be taken when using log color spaces in compositing, as not all log transforms preserve the full range of scene-referred linear code values. Most such conversions clip values above certain highlight color, below a certain shadow color, and some clip all negative values.
 
Scene-referred linear data is not suitable as input to a 3D LUT due to its code value distribution, range and the distribution of sensitivity in the human visual system as seen in Section 2.5.3. To address that problem, image data is often transformed into a log representation before a 3D LUT is applied. Some LUTs include this transform as a 1D shaper within the LUT file itself. See Appendix 4.4.3 for more detail.

ACES defines two standardized log working spaces, ACEScc and ACEScct, which are commonly used in grading.

\subsection{Plate Timing, Neutralization, and Grading}

When working with plate photography it is often useful to apply primary color corrections for each plate to neutralize the lighting across a sequence. During the shoot, there will inevitably be color drift across the shots, produced by events like the sun moving behind a cloud or the lighting moved to different locations from one day to the next. Using neutralized plates in lighting and compositing leads to nice gains in efficiency as light rigs and sequence standards track better. In most workflows the plate neutralization is done at the top of the compositing graph, the CG neutral elements are composited in, and then before the file is written out the neutralization is undone. This approach is often called “reverse out,” and its advantage is that the output delivery is consistent with the input plate, independent of the color neutralization process. An analogous set of operations are often applied with lens distortion being removed at the head of a comp and then reapplied as one of the last steps.
In some facilities, neutralized plates are written out to disk so everyone dealing with the images works with the neutralized result. This may be helpful to have if the background plate needs to be reflected or otherwise used in a CG render. The same is true with undistorted plates. It may be useful to write them to disk if a tracking or photogrammetry application, for example, can’t apply the same lens un-distortion as the compositing application. 

It is important to draw a distinction between the correction necessary for plate neutralizations, and the potentially more sophisticated Look that is crafted by the colorist. Color corrections used for plate neutralization must be reversible, and as such are typically handled as either simple offsets in camera log space or as simple gains in linear. More complex operations such as contrast and saturation are usually avoided as these type of corrections break the linearity of the plate, are often implemented differently in different software packages and will likely make compositing more difficult. In DI grading, the full suite of color operators can be used: contrast, keys, primaries, and secondaries, as there is no need to invert this process.

For additional detail on the process of conforming sequences and neutralizing plates, see 3.6.1 Conforming.XXX

\subsection{Premultiplication}

Premultiplication implies that one starts with an RGB color, adds the concept of transparency, and then premultiplies by it. This isn’t actually true. Premultiplied RGB is the natural representation for color and transparency and is the native output of renderers. A nice mental model for premultiplied RGBA is that the RGB channels represent how much light is emitted from within the region of the pixel, and alpha represents how much a pixel blocks light behind it. As alpha represents the fraction of occlusion, to maintain physical correctness alpha values must be between 0 and 1. RGB channels have no such constraints and can range from -infinity to infinity. 

Furthermore, there is absolutely no issue with RGB colors greater than alpha. This is a common misconception. Even when alpha is zero, a positive RGB color is a completely reasonable approximation of a physical process that emits light without occluding anything behind it. Indeed, this situation is common in emissive FX renders such as fire. Fire will also scatter and absorb light from the environment but it may in such a negligible amount that a zero-valued alpha suffices.

So how should unpremultiplied RGBA be interpreted? Unpremultiplied RGBA is most easily understood as, “If this pixel were fully opaque, how much light would have been emitted?” This representation is certainly useful in specific compositing contexts such as when pulling luma keys from CG elements, but as converting to unpremultiplied RGBA is a lossy approximation it’s prudent to only do the conversion when absolutely necessary.

For additional information on premultiplied versus unpremultiplied RGBA representations, see Porter and Duff (1984) and Blinn (1998).

\subsection{Technical Checks}

It is not normally practical for the workstations used for animation, lighting, VFX and compositing to show an entirely accurate preview of the final deliverable, especially if the principal master is HDR. Furthermore, while artists typically craft imagery assuming a particular viewing exposure, it is possible that the imagery will be stylistically color graded in unexpected ways in the DI grade. Therefore, care must be taken to preserve a wide dynamic range in the scene-referred linear working space, avoiding output-referred cheats and checking the result on an SDR display by pushing and pulling the exposure prior to the SDR viewing transform. Indeed, many facilities run a barrage of tests, often referred to as Quality Check (QC), Technical Checks or The Gauntlet to ensure that all completed CG and composites are robust enough to stand up to the manipulations of DI grading and that there are no artifacts which are masked by an SDR output transform that might be revealed by an HDR output transform. 

The inclusion of deliverables for HDR displays makes this even more critical, as artists’ workstations may be unable to show an HDR image at all, and even if they can, they are unlikely to accurately represent the final HDR deliverable. If HDR monitoring is not possible at the desktop, having just a single shared HDR monitor to perform technical checks during the course of work can help prevent surprises that would otherwise only be discovered late during HDR finishing.

At a minimum, artists working in scene-referred linear, namely the lighter and compositor, should aspire to view their imagery at a range of exposures as part of their regular production workflow. This enables artists to have a true sense of the dynamic range in their imagery and to match computer-generated elements with similar precision. In compositing, it is also expedient to convert the imagery to the color space, quantization, and clamping that will be delivered to DI, to test how the imagery holds up to drastic color grades. Best practice is to simulate substantial exposure adjustments, as well as contrast and saturation boosts, to confirm that all portions of the imagery track in a consistent manner. This is particularly important to match across shots and sequences. It is easy to make individual shots that are self-consistent, yet fall apart when viewed side to side. Most critical is the inspection of shadow detail. “Stopping up” the final composites, shadows should have a consistent color and density both inter-shot and intra-shot. Flat black portions of the image should be avoided - lest they reappear in DI grading - and grain/noise must match between the live-action and the computer-generated imagery. Inspecting highlights, one should confirm that their intensity, color, sharpness, and flare appearance are similarly consistent. For visual effects shots that don’t modify the entire live-action plate, the portions of the image that should be unaffected by the visual effects elements should be compared against the original live-action plate. It is important to make sure that no unintended changes to black levels, color balance, grain, resolution, bit depth or lens distortion have crept into the process.

\section{DI Grading}

Locking down the eventual DI viewing transform(s) early on is the critical first step in crafting cinematic color pipelines, as it pins down every other part of the process. The remainder of the pipeline essentially relies on visualizations that are created by working backwards from this step. 
Renders by Sony Pictures Imageworks, available for download at opencolorio.org. Images from “Cloudy With a Chance Of Meatballs” Courtesy of Sony Pictures Animation. © 2009 Sony Pictures Animation Inc. All rights reserved.
Digital intermediate (DI) grading is the process where the entire motion-picture is loaded into a digital device, for the purpose of color-grading in an environment that exactly mirrors the final exhibition.  Although the colorist may get involved much earlier in the project, DI grading is usually the final stage of post-production, where per-shot color grading is added and the visual look of the film is finalized and baked in. Color timing is the term used for traditional film lab color adjustments. The term Color correction is generally avoided as it implies that the original cinematography or VFX was flawed. However, correction is still used by colorists to mean continuity adjustments. The final step of baking in the Look, view transforms specific to different output devices plus any trims applied as a result of those transforms, is known as Mastering. As with so many terms encountered in this document the language is used imprecisely and often one term is substituted for another without the understanding of the finer implications. Whilst DI is generally understood to mean the color grade and mastering stage, it technically includes the entire post-production process and dates back to the 1990s when Kodak, who coined the phrase, were creating digital intermediate scenes for VFX then recording them back to film and cutting them into the intermediate negative, aka the interneg. Hence the term digital intermediate.

The DI grade often follows a conform of the edit decisions, which are usually made from proxies, using original source material and scene referred final effects. The conform includes final sound and titles, though these may not be available at the beginning of the session. When time is more important than quality or when the edit decision list cannot be exported, the DI grade can be applied to an export from the edit system. This is a major compromise and not recommended.

In games, the tasks and roles of the development team are not so delineated. There is rarely a specialist colorist involved and hence no DI grading session. However, games use the same or similar tools as DI grading integrated into the creation process. The nature of most games engines mandates a scene-referred workflow with a choice of output transforms to support play on any device. Consequently, Look Management is often part of the engine and far less of an issue than in the other disciplines. However there is still the concept of a mastering process, involving final quality adjustments in a review suite on one or more reference displays.

There are two main approaches to handling color in the DI grade. 
Unmanaged Output Referred 
Color Managed Scene Referred

The reference display must always be accurately calibrated and checked regularly. Color grade decisions should be made in real time to avoid chromatic adaptation induced errors. 

The first approach, unmanaged, is output-referred and began in the days of telecine and hardware grading systems. A telecine is a device that transfers film to video in real time. Telecines predate video tape recorders when the only way of recording or playing back recorded images was to use film. In the early days, recorded programs were broadcast from live telecine transmissions, but as the equipment and the standards improved, programming interfaces were added so that scene by scene grading could be applied. Programmed grades were mostly recorded to video tape recorders (VTRs) so the output was always a real-time video stream. There was no need for color management since there was only ever a single playout format and the grading accuracy was wholly dependent on the monitor calibration. The calibrated telecine monitor was often referred to as the “god monitor” so its importance was always very clear. In the modern version of unmanaged grading, imagery is loaded into a non-linear timeline with real-time playback capability; no viewing transform is applied. The image is manipulated in the grading system, akin to a motion-picture version of Photoshop, to look correct on the calibrated reference display. The grading system and the colorist using it, treat all media, video or log, in the same way, and decisions are all output-referred. Different camera image processing, exposure, transfer functions and VFX elements are generally matched and graded by eye rather than analytically. For those used to a Color Managed workflow, it might seem strange that anyone would work this way, but for decades this was the only possible approach and the resistance to change in our industry cannot be underestimated. The output-referred method is strongly discouraged. However, it it is sometimes used if there is insufficient data about the color gamut and the transfer function of the material on the timeline. Using incorrect color science can cause distortion, artifacts and clipping. See 3.3.1 Camera profiles for more detail. Output-referred grading is also used in television and commercials where DI grading is not always the last process and there is typically a single delivery format. Scene-referred grading is considered slower, more demanding and generally more expensive in some markets and because they have less time, smaller budgets and shorter-term products they are slow to transition. When there is no color management in the final DI grading session, it is much more difficult to implement accurate Look Management. Nevertheless, preview LUTs from the colorist can be used on set and in editing or grades from the DIT or editor can be used as a reference for the DI colorist.

The second Color Managed approach is usually, but not always scene-referred. For the DI grade, the reference display is pivotal to the color managed transforms. The display format is defined by the delivery specification and when there are multiple deliverables the largest gamut for which there is a calibrated display is chosen. The timeline color space should be scene-referred and all sources mapped to it. 

Sometimes the output color space is used as the working color space and transforms are only applied to the sources. With this approach grading is output-referred, and there is no real benefit to it, so it is not discussed further in this paper.

\subsection{Conforming}

The dated video-centric approach to grading comes from the time when colorists worked on a reel of film or a tape. Film and tape sources playback as a fixed linear timeline and were usually graded out of context. Media was passed from color grading back to editorial for mastering. That workflow is still used especially for commercial work, although the tape source has now been replaced by a random access digital file. The file is broken down into shots or events either by loading an edit decision list (EDL) or by scene detecting the program. The workflow is not ideal for VFX inserts but in this scenario completed VFX files are added by the editor, prior to the color grade. Titles are added after the color grade. VFX are usually submitted as output-referred files that match the delivery format reference display. This allows very little manipulation in grading and can suffer a quality loss since the pipeline is not scientifically color managed. A more flexible alternative is to discuss the process with the editor and deliver VFX log encoded, preserving more dynamic range and having more latitude in the grade. Since the final look is unknown until after the DI grade, the VFX log image, just like camera media benefits from this extended dynamic range.
 
In the recommended workflow, the DI colorist receives the edit decision list, the original source material, VFX that match the source material and the final soundtracks, and then conforms them as a multi-track non-linear timeline in the grading system. The conform might be done by the colorist, an assistant or by an editor dedicated to the task. The colorist then matches the scenes, manages the look and applies shot to shot or frame to frame fixes as necessary. The best results are obtained from a scene-referred workflow that uses color managed transforms at each stage. This brings different sources closer together before the grading begins, but more importantly, it makes it much easier to deliver alternate formats using output transforms, which are more accurate and less work than using just trims. It also permits VFX, editorial and other departments to see representative images on displays calibrated to different standards, at any stage of the process.
 
The edit decision list could be as simple as one or more EDL files, one per track, but is usually an XML or AAF that supports multiple tracks. A reference movie of the cut should be provided to check for errors in conform. These are particularly likely to occur when size and speed changes are included in the edit decision list. This list is crucial in selecting shots for VFX and determining the exact frames required. VFX should be returned with frame handles of a consistent length which simplifies conforming if there are updates. The number of frames in the handles should be agreed and specified early in the production. Mattes generated at the time the VFX are rendered are best combined as an alpha channel or multi layer EXR. Mattes created later in post production for use in DI grading can be delivered as separate files. Most DI grading systems can parse separate mattes from the RGB and alpha channels of an image file. The mattes need to be synchronized with the effect shot so it is always recommended that the matte and the image files have identical timecode or frame count. Usually, the effect shot and the matte are trimmed to the required frames, with handles if specified, and have the same duration. The file names should be identical except for a suffix for the mattes. Mattes and images are, however, usually placed in separate folders to simplify the conform process. Clearly, if two files with similar filenames and identical timecode exist in the same folder there is a good chance that the matte and the image will get confused during an auto conform.
 
In the modern DI workflow, the first conform happens before picture lock, which allows the colorist to export precise frame sequences for VFX. The colorist’s first pass focuses on neutralization, matching the adjacent shots in the cut at the time. The colorist may have already started work on the Look and more detailed select parts of the frame, but these should not be present in the export. The ideal export format is EXR, as it can preserve all the available source data. In current practice, VFX facilities prefer to have exported frames in the original camera format along with CDLs that encode the first pass neutralization. This reduces the likelihood of issues that crop up when images have been processed by different packages that may have differing implementations of transforms that should match. Input Transforms are currently a prominent source of variation between software packages. When returning finished VFX for integration into the DI conform master it is important to have a clear policy on handles, color gamut, white point, file naming and format. VFX may come from a variety of facilities around the world, so if each team use a different process they are unlikely to integrate well. Generally, VFX shots returned to the DI conform should retain the naming convention and timecode of the supplied plates with a suffix appended for versions and dates. Technically EXR is again the preferred format, but productions may choose DPX or a Quicktime or MXF wrapper for practical reasons.



\subsection{Unmanaged Output Referred Grading}

The advantage of the output-referred “video” approach is one of process simplicity. However, when grading pre-rendered gamma-encoded imagery, a relatively modest amount of color adjustment can be applied without introducing artifacts. The downside to this approach is that much of the detail in the original imagery is lost when baking-in the view transform, which typically happens before or at the beginning of the grade. Indeed, in some implementations, this baking-in of the view transform may even be internal to the camera, or in animated features, inside the renderer. For example, if a shot was originally overexposed, a sensible color correction is to darken the image. However, it is likely that in an overexposed image large portions of the image will be clipped by the view transform to a constant maximum value, and no possible correction can bring back this lost detail. Additionally, this video style approach is particularly unsuited to producing HDR versions. It is inherently tied to the dynamic range of the display used in the DI, so the HDR and SDR versions must be done as completely separate grades.

\subsection{Color Managed Scene Referred Grading}




An original log film plate (top-left) is loaded into the color grading system. When viewed with a film emulation table (top-right), the appearance is predictive of the final output with a default exposure. If the exposure is lowered using an additive offset in log-space (lower-left), new details in the flame highlights are revealed. When a similar adjustment is applied in an output-referred space (lower-right), this detail is lost.
 Imagery from “Spider-Man” Courtesy of Columbia Pictures. © 2002 Columbia Pictures Industries, Inc. All rights reserved.
Going back to the overexposed example, all of the dynamic range from the original camera capture is preserved when using a scene-referred approach. Thus, when the exposure on log data changes, new details which had not previously been visible through the view transform may be revealed as no clipping has occurred. This allows color grading in log DI to be very high fidelity and most trims do not result in a flat black or flat white in the image.

In the color managed approach, scene-referred imagery is loaded into the machine along with a viewing transform specific to the target reference display to create the final Look. The color grade manipulates the underlying scene-referred representation, but all color judgments are made previewing through the output transform. It is increasingly common to create an approved master for the main delivery format and then to use a trim pass in conjunction with each different output transform required. The trim pass can be an additional grade or metadata.

Camera raw images are the preferred choice of the colorist for the master grade. Failing that a camera-specific integer log encoding is utilized. Camera raw images are typically recognized by the main DI grading systems and facilitate the use of the correct camera transforms.

DPX was commonly used as a log delivery format to DI and both 10 bit and 16 bit integer versions were common-place. For grainy material 10 bit is often sufficient. DPX is still used for film scans and integer sources such as a camera debayering. When passing true scene-referred linear renders directly to the DI grade, 16 bit floating-point EXR is preferable. 16 bit integer linear representations are not recommended.

All DI grading systems process floating-point internally and can happily read EXR files, which have replaced 10 bit DPX as the preferred interchange format. However, the size of EXR and 16 bit DPX is currently costly for storage, processing, and bandwidth, so their use is far from ubiquitous, and some productions still use 10 bit DPX, even though it is less than ideal. 

For viewing, a suitable output transform for the target reference display is applied. The DI grading transform should match the Look Management transform. The color managed workflow should be agreed on and shared with everyone on the project, including those on set and during production to maintain consistency of the images and avoid the need to bake in the transform until the delivery stage. The scene-referred working format is an excellent archival master. The graded scene master can generate multiple deliverables and is the main asset but un ungraded version is often kept too. More information is available in the ACES Digital Source Master specification. Resolution and bit depth of all work should ideally be at least the same as that of the camera original media.

For a simple, robust color managed workflow, ACES as described in 3.1 Academy Color Encoding System (ACES) is ideal. ACES is supported in a wide range of software applications and the standard input and output transforms are reliably consistent. ACES uses EXR files to store scene-referred data that encompasses all of the visible spectrum.

When film was the primary theatrical deliverable, the viewing LUT used to be matched to a specific film lab and print stock, but that is rarely true now. Today theatres and broadcasters expect digital masters and the reference monitor should use the largest color encoding of all the deliverables. For the theatrical exhibition that is usually DCI P3 but for most broadcast deliveries it is still Rec. 709. VOD platforms and games devices are targeting HDR displays, which are currently limited to P3 color gamut with D65 white point, delivered as Rec. 2020. There are several flavors of HDR but they all use either PQ (Perceptual Quantiser) or Hybrid Log Gamma transfer functions. It is quite simple to convert between the two, but since PQ is by far the most common in practice, mastering to a PQ reference monitor, P3 gamut and D65 white point are recommended. All deliverables can come from that. Whatever the output transform is, it is highly recommended that the master is created by color grading the image in a scene-referred color space and making visual decisions based on the appearance after the output transform.

Standard practice is to start with the DCI theatrical master even though it will not always serve as a universal master for other deliverables. Artistic intent and the bulk of the creative decisions are inherent in the theatrical master so the DI grading project with a broader output transform can be re-used as a very efficient starting point for the universal master. Dolby Vision cinema, Eclaircolor and Samsung Onyx masters can only be color graded in a theatre on a screen of the same type as the target cinema. These deliverables are only ever created in their final viewing environment.



Artistically, the DI process can be segmented into a per-shot correction that neutralizes shot to shot color variation, and then a secondary grade that crafts the overall artistic look of the film. With those two adjustments as a base, further work is carried out to manipulate audience experience of the scene and to address any distractions or inconsistencies. As a rule colorists work on sequences, though sometimes with dynamic changes in a shot. Any work that needs to be done frame by frame is usually best done as VFX rather than DI grading. It is common for the DI facility, if an initial grade happens early enough, to communicate to the VFX houses the Looks being used. This often is sent as a CDL, or a 3D LUT, per shot and does not have the viewing transform baked into it. It is also important to communicate the color space that the color grades have been applied in as the CDL or 3D LUT will not produce the intended result without this information. Most often, the DI grading reference display is calibrated to a color space that is the same as the delivery format. 
Key Points
A color managed scene referred grade is recommended
Grades should be applied to a conform of original camera files
Multi layer EXR files are the preferred interchange format for VFX and mattes
Separate matte files can be used if the master image is unchanged
All sequences exchanged in post-production should have fixed frame handles
All sequences exchanged in post-production should retain existing metadata


\subsection{DI Color Grading}

The final DI grade takes place after the conform, but often before picture lock. If possible the colorist starts the grade before VFX are ready and expects to add them to the conformed timeline along with further editorial changes. The grade has three main objectives. First, it must match consecutive scenes seamlessly, even though they may come from different sources or different color gamuts. Second, it is the final stage before delivery and the whole project, not just the images must look the best it can. Whilst the phase is known as DI grading, the DI colorist acts as a crucial QC of the finished master. The DI grade may be the only environment to have large, calibrated displays and might also be the first time that final audio, VFX, titles and composites are seen under critical monitoring conditions. In the ideal world, problems would be sent back to the appropriate team, but as red carpet day looms it is often the colorist who has to fix any remaining problems, even if those problems are sound sync, reflections of the crew in shot or errors in the credits. Thirdly, most important of all, the grade must enhance the concept by communicating emotion, environment and clarity. Like VFX, the best grades are often the least noticeable yet still manage to add depth to the narrative or content. Early on in the DI process, the colorist, editor and VFX departments collaborate and interact. A post-production supervisor might manage the interaction, but as deadlines loom, remaining changes are increasingly left for the colorist. It saves time and helps to keep things running smoothly if files are exchanged in an orderly way.

The project Look is designed to some degree in pre-production. In animation, the look is usually established quite early on. In games, the look evolves as the game develops. In DI grading the Look can be established and communicated through a common transform or show LUT or it might be left until the final DI grade itself. If the colorist is given access to the scene-referred source files and information on their color gamut, a well-managed color pipeline can and should be used through the final mastering stages. Raw camera captures usually contain sufficient metadata to identify their encoding and color space. However, if the files have already been encoded to a more manageable wrapper, ProRes, for example, problems can arise. Colorists often receive files that are either not scene-referred, or the information about their true origin has been lost along the way. When this happens, the colorist can only use experience and good judgment to grade the project and the opportunity to finish through a properly color managed workflow is often lost. If it is absolutely necessary to simplify the DI grade by working from a conformed source in a compressed wrapper format instead of the original source files it is crucial to preserve information about the source and history of  each shot and useful to ensure that all scenes are correctly transformed into the same scene referred working color space. Failure to do so can result in problems and a loss of quality that should have been easily avoided.

When EXR source images or camera raw files are delivered for DI grading the established Look for standard dynamic range is used to begin the initial grade for digital cinema. The source images may be supplemented with additional matte images created by VFX that allow individual characters, or even specific elements like hair or eyes, to be isolated for color grading. Following the completion and approval of the digital cinema grade, a trim pass is performed for theatrical 3D exhibition. Typically this will involve an increase in contrast and saturation to compensate for the lower projection brightness in this environment. Next, a trim pass is performed for theatrical HDR. The standard range look is swapped out for a matching look with extended highlight range. Because theatrical HDR is capable of much darker blacks than standard digital cinema projection, black levels may be further adjusted during the trim pass to maintain appropriate levels of contrast. Rec. 709 for home deliverables is also derived from the digital cinema grade as a trim pass. In this case, a gamma adjustment is made for the change in the viewing environment. Also, the Rec. 709 color gamut is smaller than the digital cinema P3 gamut so some amount of gamut compression may be necessary to preserve image details that would otherwise clip. Lastly, a home HDR grade is performed using the digital grade as a starting point, but substituting an HDR look that targets the much brighter highlight range, color and shadow detail achievable on HDR televisions. The home HDR grade is followed by an analysis to generate HDR metadata. For deliverables that require dynamic metadata, such as Dolby Vision, the HDR master is reviewed by the colorist on different displays and further metadata adjustments recorded.

There is a range of grading systems in use today, and very few of their tools are interchangeable with each other, let alone VFX software. This makes it a challenge for VFX teams to apply reversible transforms or to view accurate simulations of the DI grade without baking it in. The most common methods of previewing VFX are LUTs and CDL. Most color grading systems can export grades as a LUT, however, the limitations of LUTs apply. The two main limitations are that they apply changes to the whole image, with no ability to apply specific changes to selected areas of the image, and that they often clip or produce interpolation errors. See the Appendix 4.4 LUTs and Transforms for a more in-depth discussion. 

Whenever possible, it is best to preview shots in a grading suite or theatrical environment as a confidence check. When there are multiple facilities contributing to different aspects of the show it eventually comes down to getting together in a color suite with a calibrated and controlled environment to ensure that all of the moving pieces do in fact work together. For example, if two VFX plates are cut together and one is slightly warmer, removing red would make a purple suit not match the following shot, so the purple needs to be adjusted separately. Adjusting individual elements through the use of mattes, keys and shapes is called secondary correction. This goes for the sky, skin tones, background and more, even without an overall change in direction of the Look. Often the Look continues to evolve after production and VFX right up to the finishing of the project, which happens in the grading suite.

Key Points

\subsubsection{DI Grading Tools}

Color grading tools have evolved with technology. In the early days of film, adjustments were limited to exposure and color tinting. With color film came color timing with RGB printer lights, which are quite literally filtered lights to alter the color of the print copy. In the video era, analog and then digital tools were developed to optimize film transfers to video via a telecine. The DI grading software of today typically copies or emulates the historical processes, and adds new features from computer graphics, VFX, digital cameras and improved color science. New software grading tools appear slowly, often a long time after the technology that requires them. At the time of writing, there are few special tools for HDR display grades for example. Colorists consequently have become adept at repurposing tools to fit new needs. Grading software has become sophisticated enough that it is rarely possible to recognize the exact tool or software from the result. However, there are significant differences in alternative approaches to the grade. These differences must be considered part of the creative process but a review of the most common techniques can help to understand how a look is crafted and how look management is integral to the process. In short the tools used and the order in which they are used, affect the outcome.

The following section discusses tools and workflows in more detail as the workflow, or more specifically the working color space the tools are used with changes the effect of each tool. Some systems have tools that attempt to achieve the same behavior in each different color space, although that mandates a full color managed approach. Other systems have less sophisticated solutions in which the range of a tool is altered according to the color space and many systems have controls that can be tweaked by the user as needed. However, the most basic commonly used tools are surprisingly agnostic about where they are used, and consequently, they feel and react differently in different workflows. The controls are written with a capital, e.g. Offset, to distinguish them from pure mathematical operators e.g. offset.

For simplicity, an unmanaged color workflow reveals the raw nature of two different grading tools. An unmanaged workflow is a workflow in which the source color space, the timeline color space, and the output color space are all the same and so no transforms are applied by the system. This is still the default for many DI grading systems and many colorists continue to work this way since it is easier, usually requiring no project set up at all. This section, therefore, describes color grading tools as used in the older WYSIWYG approach and is output-referred. This is not the recommended approach but serves to best illustrate the nature of the toolsets and how they are used.


ARRI LogC source image with references. Image © Geoff Boyle All rights reserved.


For the purposes of discussion, the above test image is processed in different ways. The source image has a deliberately wide dynamic range captured with an ARRI camera as ALEXA LogC. The color chart and the two faces at the top of the image are from the camera. Bottom left is a waveform representation of the processing applied to the image and below that a grayscale ramp from which the waveform is derived. The waveform does not reflect the image itself, only the processing applied. Bottom right is a vectorscope display of the color chart which was part of the camera capture. The vectorscope only shows the chromaticity of the chart after processing and is a useful way to compare saturation and hue shifts. The image above is unprocessed so the waveform and ramp show a true linear response. The following examples are not color graded in the true sense of the term, they are examples of the starting point for a DI grade. This type of adjustment to get a reasonable starting point is sometimes called the Technical Grade or a dailies grade. On a real job some color balancing is often needed too, but for this test, neither color balance nor saturation was adjusted. To obtain a realistic comparison the test image was graded using a limited toolset with a waveform as a reference. The grade sets a technical black level and enough contrast to use as much of the available dynamic range as possible, whilst retaining a pleasing result on both pale and dark skin in the same image on a calibrated Rec. 709 display. 

These examples are not exhaustive, there are countless variations possible. The purpose of this exercise is to underline the absolute need for good Look Management. The above image is scene-referred, so the real world examples shown here are intended to demonstrate that using no color management or different color management causes the Look to change significantly. At best, this results in more work to maintain continuity. At worst the intended Look must be compromised or completely re-thought in order to deliver high-quality images. Goo d Look Management is all about consistency.

The three trackballs that make a grading control surface so recognizable are initially mapped to Lift Gamma and Gain (LGG) in most systems. Since these are the first available controls it is not surprising that they are the most commonly used by many colorists. The actual math behind the LGG set can differ between manufacturers, and a few systems offer several versions, but these tools originate from the telecine era of transferring film to video. Since film stocks have different response to light in the shadows, midtonesmid-tones and highlights, colorists needed separate controls for them to manage the film dynamic range in the much more limited video gamut.

The Lift or Black control pivots around white and affects all code values except white. The definition of white is typically code value 1023 in a 10 bit range but again can vary among tools and systems. The tool is ideal for balancing blacks in any color space because the darkest tones are always affected more than the lighter tones, but it is not very selective. A characteristic of this tool is that it pushes the darkest parts of the image, the blacks, harder than any other tone, so when lowering the black point it is prone to causing compressed or clipped shadow detail. In look creation, sometimes this is desirable and sometimes not.

Gain is similar in use to Lift but pivots around the black point or 0. Again different tools and systems can define the black value differently, but if the black point is anything above 0, further conditions need to be defined so that values below black are not affected. When increasing Gain to brighten an image, the brightest values are the first to be clipped or compressed. 

The Gamma control varies between systems far more than lift and gain but is always a simple curve that distributes values towards shadows or highlights. It should normally not affect the absolute black or white level. In many systems, the Gamma control has a fixed range that is output-referred. Consequently, when used on a log source in a scene-referred workflow the code values that become black and white levels at the display are well within the range of the Gamma control and do affect blacks and whites. For this reason, many colorists avoid a simple Gamma control altogether.



ARRI LogC image graded with Lift, Gamma, Gain, and no color management. Image © Geoff Boyle All rights reserved.

In the test image Lift was used to lower the deepest shadows to black on the waveform and Gain was used to bring the brightest details in the flowers to white on the waveform. A small Gamma control adjustment was then applied for a pleasing start point. Without color management, the saturation remains low and there is better hue separation in the warm colors than the cool ones. The result is a quite contrasty, hard looking, low saturated image. The waveform and ramp show clipping, but since none of the values in the test image are in the clipped range that is not a problem. However, it is worth noting that the definition of the white clip point is arbitrarily chosen by eye in this example. Setting the whites to clip in this way is the beginning of crafting a look.

When DI grading was first introduced around 2000, there was some controversy over the use of these video tools in a film to film process. In film timing, there were less sophisticated tools and many felt that in order to retain the essence of the negative to print chemical process, those tools should be more closely emulated. Interestingly, the resulting DI tools, Offset and Contrast, did not exist in any of the video hardware color grading systems, though there were comparable tools in the telecines themselves.

Offset is an additive function applied to all the image code values, so on its own, it does not differentiate any tonal range from another. It is confusingly labeled Offset, Black, Lift (incorrectly), Exposure, Density and probably some other terms according to the system in use. 

In scene-referred linear, a gain operation is typically used to change the color balance and scene exposure. In log space, this roughly corresponds to additive offsets. If a mathematically exact log is used, they are in fact identical, though most manufacturers tweak the log encodings as previously mentioned. Log offset color adjustments per color channel are ubiquitous in motion-picture industry color grading and often referred to as the “primary grade”. Theatrical “fades to black” have a very particular appearance to them, which is a direct consequence of the fade being applied in log space, as viewed through the traditional “S-shaped” film emulation. Fade to blacks applied as gain operations in scene-referred linear have a roughly similar visual appearance, with specular highlights lingering on screen well beyond mid-tones.

The Contrast tool in a workflow that is not color managed is usually an s-curve with an adjustable pivot, applied after Offset. The contrast tool pushes low mid-tones towards black and high mid-tones towards white and approximately simulates the response of a film print stock. It has a less aggressive look than lift and gain and is similar to a color management tone map curve. In color managed systems the output transform is likely to include an s-curve and the Contrast tool might then be a straight line pivoting around an adjustable mid-point so that the two do not conflict. The overall visual result is still an s-curve.


ARRI LogC image graded with Offset / Contrast and no color management. Image © Geoff Boyle All rights reserved.

In the test image, Contrast (S-Curve) was used to adjust the dynamic range to the display and  Offset to bring the range out of clipping. The contrast pivot point was adjusted to maximize the difference between shadows and highlights. Once again this was a purely technical approach based on the image content. Interestingly, this approach produces an even harder look and struggles to get a good representation of the darker skin. A lower contrast setting would work better for both faces but would have left the image looking milky with either lifted blacks or whites below peak. The waveform and ramp show that there is no clipping, which would make it better for a viewing LUT. Chromaticity is only slightly changed from the LGG example. When Offset is used with Contrast, the Contrast control shapes the results.

There is a third type of control found in many systems, that consists of three tools often described as shadows, mid-tones and highlights. These tools are more selective and curve away from a pivot point. The pivot points may be user-defined and the shadow, mid-tone and highlight ranges may or may not overlap, The result is something in between LGG and Offset/Contrast. A single grading system may have just these basic two tool sets or many more variations to choose from. In some systems, the tools are modified slightly by the project settings for color space, and in others, they can be redefined each time they are used. Where there are many variations of the primary tools, the processing order is important and again systems vary. Sometimes the processing order in each layer or node can be altered and sometimes not. When the processing order of tools is fixed, the colorist needs to use more layers/nodes to achieve the same result.

The above tools are all RGB. The 3 channels can be ganged together and used to manage brightness levels. This is called the Master control by colorists. Alternatively, each color channel can be adjusted separately either by a ball or circle or with a knob or slider. Adjusting individual channels allows balancing or neutralizing a color bias but is also used to create a look or style. Three channel RGB tools are commonly referred to as Primaries, not to be confused with RGB color space primaries.

In output-referred grading these primary adjustments are made to bring the image to a viewable state on the reference monitor in lieu of an output transform, whereas in scene-referred grading their result is subsequently processed through the output transform, which alters the effect of the controls. The decisions are based on aesthetics but intended to be neutral. The only definitive information in the image is an unexposed black, the precise values of all other tones and colors are unknown in the absence of Look Management.

The colorist crafts the Look after the initial technical grade using more refined and sophisticated tools. Secondary controls are used in conjunction with external mattes, keys, and shapes to achieve selective changes. The DI grade is built up in layers or nodes, depending on the interface, which can be interconnected and blended together. The secondary controls can be used as part of the Look, or to modify it. For example, if the Look is dark and moody it is common practice to use secondaries to brighten skin tones.

\paragraph{External Mattes}

Sometimes, grading will require external mattes for CG shots, so it is advisable to discuss the need for alpha channel or separate mattes ahead of delivery. The mattes used for grading may not exactly match those used in compositing and can, therefore, incur additional costs.



A sampling of mattes from WreckIt Ralph 2 used to enhance DI grading. 
Image ©Disney. All Rights Reserved.
Animation especially can generate and utilize large numbers of mattes allowing the colorist to isolate individual characters and even specific features such as hair and eyes for fine corrections. It is not unusual to have more than 30 mattes per scene in animated features. Mattes also allow the colorist to separate foreground and background elements, and to rebalance lighting, further improving the continuity of shots even after rendering has been completed. This can save time that would otherwise be spent on multiple render iterations. When the Look has already been established early in animation pre-production and there is a process in place for notes to be recorded during production lighting reviews, the colorist generally starts from a position that is very close to the expectations of the art directors and can focus time on addressing the specific notes that were recorded during reviews.

\subsubsection{DI Grading Workflows}

There are as many workflows for DI grading as there are for VFX. Some start as early as pre-production, some involve live grades during production and some do not assign the finishing colorist until quite late in the project. Collaboration between VFX and the Color department always pays dividends. If the grade begins after VFX, which is not unusual, then plates are likely to be delivered as camera files and need neutralizing. Without a clear direction of the final look, flexibility and latitude are even more critical. If a colorist is on board in time, look creation will have at least begun, and it is reasonable to expect neutralized plates from the Color department. The file format and viewing arrangements can vary according to time, budget and complexity, but scene-referred plates without the look applied are recommended. An ACES EXR file is the most reliable option.  While CDLs are good for providing a general direction for the Look, they are quite limited in scope. A 3D LUT can achieve more selective changes but is less useful in communicating alterations. It is possible to use a CDL and a LUT providing all parties can handle both. There are bespoke and proprietary tools which work well, are more precise and better for communicating change, but again only useful if all parts of the process are compatible. Even with good planning, seeing shots on a big calibrated screen in a controlled environment often reveals things for the first time and adjustments have to be made.  Highlights that distract from the action might need dulling, or an important part of the image might need enhancing to bring more attention to it. These changes might cause a ripple effect and other elements might then need matching back to the new tweaks. Color finishing is fluid and ultimately where all the pieces of a project come together.

In animation, Look Management is usually settled at the start of production rather than at the end of post-production. The Look is much more likely to remain consistent up to and including the mastering stages of the DI grade. The DI Grade for animation is expected to focus on scene-to-scene continuity and smaller adjustments to highlight story points, less so on crafting the full look of a sequence or project.

Look Management should determine the methods by which elements are monitored and quality checked. It is, of course, appropriate to use waveforms, vectorscopes, and histograms to measure scene-referred values, but it is often reassuring to also see something output-referred. Since different approaches for output-referred previews produce very different results, a common method is invaluable. The test image from the previous section Section 3.6.4.1 DI Grading Tools was also processed with different color management solutions to demonstrate the risk of mixed approaches.


ARRI LogC image with ARRI LUT applied and no color correction.  Image © Geoff Boyle All rights reserved.
This first example is the most obvious and in some ways the truest to capture. It uses a LUT supplied by the camera manufacturer to optimize chromaticity and prepare the image for the EOTF of a Rec. 709 display. The ramp shows that the LUT tone maps all possible values without clipping. The result is better chroma separation and saturation than the primary grade examples, but also a flatter image. In fact, this representation is probably the most accurate and would serve as a good starting point for grading. In the DI grade color trims can be applied before the LUT, which keeps the process truly scene-referred. However, it is also common practice in television grades to color grade after the LUT. Grades after the LUT are output-referred and, as discussed previously may limit the ability to recover highlight and shadow detail. Additionally they cannot be used with a different output transform to target other display formats. 


ARRI LogC image with the ACES Rec. 709 Output Transform applied and no color correction.  Image © Geoff Boyle All rights reserved.


This is the result of the test image output from ACES after the correct ARRI Input Transform and the Rec. 709 Output Transform (OT). The result has more punch and slightly more saturation than the ARRI LUT. The S-curve tone map is steeper and shows a different response in the shadows near black. In an ACES project grading is always scene-referred because the OT is managed by project settings rather than as a grading node. This lack of ambiguity is one more reason that ACES is considered the simplest, most reliable form of color management by so many. A cinematographer using ACES to monitor on set can be certain that VFX and DI grading can display the images exactly as seen when shot.


ARRI LogC image with simple color management and primary grading.  Image © Geoff Boyle All rights reserved.

This image is the result of a simple color management system in which the source and working color space are set to LogC ALEXA Wide Gamut and the output is set as 2.4 gamma Rec. 709 and all grading is, therefore, scene-referred with minimal transforms. Since there is no tone mapping applied by the color management it was necessary to use a small Gain adjustment to bring the image out of clipping. This was done by taking the brightest pixels, the highlights on the flower, and bringing them down just out of clipping. The resulting image has the lowest contrast of all the tests and shows details even in the dark skin. The shadows on the pale skin are so light that the look is rather unnatural. The chroma separation and saturation are well defined and clean. This would make a decent starting point for a commercial or a Look for beauty products but because the color management alone does not tone map the entire dynamic range to that of the display. Exposures higher than the test image would appear clipped on a Rec. 709 display, even though the source data might not be clipped.

The results from using different color tools or color management would cause for confusing conversations at best, but the greater concern is that finished scenes could be hard to match in the final conform because although they were created scene-referred, they were seen differently when they were created. Even the decision to apply tweaks to an otherwise standard color managed pipeline is a material fact that can considerably affect the time taken to achieve the DI grade and can in extreme cases necessitate a late change to the established look in order to hide discrepancies that were not seen in the creation stages. 

The above examples are not in any way exhaustive but clearly demonstrate the importance of Look Management. All of the examples are valid viewing transforms without an intentional creative Look applied. Any of these methods might be chosen as the starting point for crafting a look. Considerations in making that choice range from repeatability across multiple software, tone mapping capabilities to different display formats, and the ability to display good saturation and contrast, to the best aesthetic choice for managing the two skin tones in the same frame or preserving the hair color most accurately. These comparisons are kept simple for demonstration purposes. Real world examples could include considerably more effort to include an established Look on monitored previews.
XXX

\section{Critical Monitoring}

DI grading is done in a viewing environment that exactly mirrors the final primary exhibition. Digital intermediate (and mastering) typically have very tight tolerances on calibration, as the decisions made in this process are the last time image color appearance is tweaked.

An issue in some feature production is that the typical artist display does not match the eventual theatrical viewing specification or environment; some audiences will see more color detail than the original artist. Workstations commonly use sRGB or Rec. 709 displays, which represent the smallest color space and lowest dynamic range used in distribution. Workstation displays are often not even calibrated beyond an eyeball check. Theatrical releases use the slightly wider P3 color space and there is a fast-growing home market for HDR content, which also uses a P3 limited color space but has considerably more dynamic range than traditional displays. Streaming companies like Netflix are increasingly producing original content in HDR.

Histograms are a useful tool when analyzing image fidelity, and often help forensically track down if post-processing has been applied to an image. For example, log frames from certain motion-picture cameras often do not make full use of the integer coding space. This can be detected in the histogram and is helpful in tracking down an image color space when the original is unknown. The application of sharpening filters, detectable from their residual undershoot and overshoot, can also be observed in the histograms. It is possible to detect if image quantization has occurred, appearing as tell-tale “comb-like” peaks and valleys in the histogram. Leveraging histograms computed on HSV-transformed imagery as a final quality-check has been proven to be an invaluable tool.

There are other image artifacts, such as aliasing, that are best inspected on moving imagery. For example, the use of interpolation kernels, such as nearest-neighbor or bilinear, are often unobjectionable on still imagery yet artifact-inducing on slowly animating transforms, particularly near horizontal lines. Always be on the lookout for moire patterns and other high-frequency artifacts, particularly when such filters have been applied to imagery with fine details, such as cloth, or wave patterns. If all digital imagery were ideally sampled, this would not be an issue. However, camera manufacturers have a tendency to bias imagery to be overly-sharp and motion transformations accentuate such artifacts. Over sharpening is less common with high-end digital cinema cameras.

For stereo deliveries, it’s critical to quality check both eyes individually, as well as together in a proper stereo viewing environment. There are many stereo artifacts which are only visible on true 3D displays, and catching these early on is most cost-effective.

\subsection{Cinema Projection}

Mastering for digital cinema should be done in an equivalent theatrical setting with matched digital projection. Film print is rarely the primary release today, but if the project does call for it, the DI grade is usually in a theatrical environment with a digital projector, with a film emulation 3D-LUT made in conjunction with the film lab to confirm a visual match with the stock, chemistry, and processing. Producing a calibrated film LUT is expensive and time-consuming, so it is more common to create a DCI P3 master and match the film release to that. 

If possible, attending the grade or an early test screening is informative and educational.

\subsection{Reference Monitors}

Home theater mastering should be done in an “idealized” home environment. Usually, the idealized home environment is slightly darker than the real home environment and display gamma of 2.4 is used to compensate. When seen in a room with more ambient light the master will look correct with 2.2 display gamma, which is the target for consumer televisions. The purpose of the darker environment for mastering is to make shadow detail and colors more discernible, thereby creating a more accurate master. Rec. 709 does not specify the display gamma but this was later clarified in BT. 1886.

Note that whilst sRGB and Rec. 709 share the same primary color coordinates, the display gamma is quite different and decisions on tonal detail should not be made on one for the other. The purpose of sRGB as a preview color space is more to assess the content than the look.

Similarly, many motion pictures designed for theatrical release are finished on a calibrated monitor rather than a projector. Many of the monitors in use calibrate well to DCI P3 and the room lighting is often close to the theatrical experience. In this situation, the main cause of problems is the size of the screen and only experience will tell how the perception of the image will differ on the big screen. Techniques such as vignetting work well to focus attention on a small display but can reduce the sense of space in a theatre, reminding the audience that they are in a dark room watching a screen.

A final obvious consideration is the display calibration: cheaper displays drift quicker than more expensive ones, but all displays used for finishing and critical decision making need regular checks and calibration. Often calibration is done using display menus, but calibration LUTs, loaded into the display or an external LUT box can be quicker and more accurate. For a calibration LUT to be effective the display's native gamut must exceed that of the target.

\subsection{HDR Displays}

Consumer HDR displays are commercially available today and content can come from video streaming, Blu-ray players, games machines and even live transmissions. The professional HDR displays are extremely expensive, often 10 times more than conventional SDR displays, and therefore rarer. The brightest consumer displays reach about 1000 nit. The brightest professional reference monitor is currently 4000 nits but only available through Dolby. The brightest commercially available professional reference display is currently the FSI XM310K, which reaches 3000 nits. There are several reference displays that are 1000 nits or better and that meet the other criteria for HDR. HDR display mastering must be done on a display of at least 1000 nits, with full P3 gamut and a white point of D65. Scene-referred sources, VFX and CG content should preserve as much highlight range as possible in order that the mapping to the display range can be chosen for creative reasons, rather than imposed by technical limitations. Clipping generally looks bad on HDR displays. 

\subsection{Color Management for Mastering}

Ideally, a single Master is created and all versions taken from it using metadata, transforms or trims. To achieve the best results the Master should be created and approved in the highest dynamic range and widest color gamut, but even then care is needed to map the Master data to each version. A poor mapping may produce artifacts that range from minor to catastrophic but is always avoidable.

Sometimes it is unavoidable that the first Master is not the ideal dynamic range or color gamut and sometimes it is limited in other ways, such as bit depth or lossy compression. In this situation, the best route to delivering other versions is to go back to the project file and remove the embedded limitations of the first Master. A typical problem in this scenario is managing expectations when approval has been given for a Master of limited range. Some clients insist on a faithful transition between formats, even if the limitations of the approved Master are carried over to the other versions. Others are more open minded and prefer to forego absolute accuracy in favor of higher quality versions.

When media is used in a scene-referred workflow it is important to manage detail and artistic intent through the pipeline. The following details from the Disney movie “Moana” illustrate how color management is used at each stage so that tonal information is preserved throughout. 

Image ©Disney. All Rights Reserved.
This is a representation of an extract from the scene-referred linear EXR source. Since there is no tone mapping much of the tonal information in the shadows and the highlights appears clipped. It is a useful scene-referred image, but hard to monitor in a meaningful way without tone mapping and the display transform approved for the project.


Image ©Disney. All Rights Reserved.
This is a representation of the same scene-referred linear EXR with 4 stops less exposure. Most of the image is now close to the display black point, but of course still, present in the EXR file. Dropping the exposure shows that there is good highlight information in the image and that the dynamic range is greater than the display can handle without some tone mapping or clipping.


Image ©Disney. All Rights Reserved.
This representation has 2.2 gamma applied. Gamma brings the shadow detail into range on a Rec. 709 display, but information above 1.0 is still clipped. This is evident in the eye of the fire goddess, Te Fiti, the brighter parts of the sea and the sky. 

Image ©Disney. All Rights Reserved.
This is the EXR file with 2.2 gamma and 4 stops less exposure. Again it reveals that there is highlight information but it is a poor representation of the artistic intent. 

Image ©Disney. All Rights Reserved.
Here the image has had a PQ EOTF added and the color gamut converted to Rec. 2020. Some tone-mapping has also been applied. On an HDR display the full impact of the color and dynamic range are visible, but the image looks flat and desaturated on a 709 display. Color encoding to high-dynamic range and a wider color gamut uses lower code values, with less color separation, than would be used by Rec. 709 for the same display colorimetry. In this example, the detail in the highlights of the water is retained, but the detail in the eye of the lava creature has been limited for artistic reasons. The scene-referred linear EXR clearly has more detail in tone and color than is shown in this PQ image and the PQ  Rec. 2020 encoding certainly has the capability to show that detail. However, the effect on the final audience is not based merely on a technical transform and the colorist must consider the emotional response too. The important factor is that an informed decision is only made if the detail is present in the source. If there was no detail in the original EXR then the more limited version is the only option and that could impact the look of other scenes or even the whole project.


Image ©Disney. All Rights Reserved.
This is a colorimetric Rec. 709 transform with 2.2 gamma of the PQ Rec. 2020 version. Again, this results in clipping, not just in the eye, but also the sea and sky, as it uses picture information from only the lowest part of the HDR range. The clipping goes beyond the white highlights but also manifests as over-saturated loss of detail in the blue of the wave.


Image ©Disney. All Rights Reserved.
This image is also from a Rec. 709 2.2 gamma export, but this time using creative tone mapping. The artistic intent of the bright eye is retained, even to the point of a slight yellow tint. This version retains all the detail of the sea tonal range and color separation without clipping.


Image ©Disney. All Rights Reserved.
A butterfly wipe between the tone mapped version (top) and the clipped transform (bottom) shows the benefit of mapping over blind transforms. Sometimes tone mapping is achieved with mathematical algorithms and sometimes this level of control is the domain of the colorist.

\subsubsection{Gamut Mapping}

Tone mapping is concerned with dynamic range, which can affect color, as the previous example showed. Gamut mapping adjusts colors between different color gamuts. Understanding how to read files without a display of the correct gamut is a useful skill.

Image ©Disney. All Rights Reserved.
This is a crop from the original P3 gamut source. It will only look as intended on a P3 calibrated display. On a Rec. 709 or sRGB display the image will appear less saturated than it should, but the color detail is visible.


Image ©Disney. All Rights Reserved.
Here is the same crop, converted to Rec. 709 with clipping in the red channel evident on the cheek, shirt and background. The saturation level is now correct and the transfer function and white point remain the same but the clipping artifacts are objectionable and an unnecessary compromise since there is no clipping in the P3 source. The clipped colors are valid in the P3 gamut, but out of the Rec. 709 gamut.


Image ©Disney. All Rights Reserved.
This version shows a gamut warning. The zebra pattern identifies all areas of the P3 source that are outside of the 709 gamut and helps to direct the artist to where artifacts will occur if no further action is taken.


Image ©Disney. All Rights Reserved.
This version shows the Rec. 709 gamut master after gamut mapping the colors that were outside of the Rec. 709 gamut to appropriate colors inside the Rec. 709 gamut to avoid clipping in the red channel. The difference is subtle on the printed page but significantly better than the clipped approach. Most colors that were already visible in the Rec. 709 gamut remain unchanged, only the out of gamut colors and those with the highest saturation are different. The result has minimal impact on artistic intent, retains the highly saturated look and yet avoids a loss of detail in the 709 version. 


Image copyright © Disney 2018
This butterfly split of the clipped image on the left and the mapped image on the right shows the improvement in the cheek and the hem of the shirt. Had the original master been the Rec. 709 version on the right, the P3 version would look identical to the Rec. 709 version when seen on a P3 display, though it would still look less saturated when seen incorrectly on a Rec. 709 display. No gamut mapping would be required. However, the opportunity to push the saturated look that is clearly intended, to the limits of P3 would be lost.

\section{Finishing}

It is common in DI to create masters for multiple output devices. On major motion-pictures, masters for digital projection, film release, home theater in both HD and standard def, a mix of 2D and 3D, and most recently in wide gamut HDR are expected. The general approach is to identify one output as the gold standard and to spend the majority of the artistic time grading the images to look perfect on that device. For cinema, the theatrical digital projection is most often the appropriate master to treat as the reference. The director, producers, and other creatives will be present at this process. Once the main color grade is complete, additional masters are handled as trim passes atop the main output. Trim passes are most often implemented as additional color layers or nodes added after the master grade, and only utilize minor adjustments such as contrast, brightness, and saturation. The display transform baked into each master is device-specific; a different view transform tailored to each output device is a necessity. Where it is for conversion to a smaller colour space, and involves no change of dynamic range, it may be possible to perform the trim pass in an output-referred space. Creatives do not need to be present during the creation of trim passes, as the colorist is already familiar with the final look of the material and has a known reference to use as a color appearance target.
To support deliverables for standard displays as well as next-generation displays, a workflow utilizing the EXR image format can be used. A creative look and tone mapping are followed by a display transform for viewing. For HDR deliverables, the standard look has a complimentary HDR look that can be swapped at any time for review or grading on an HDR display. This allows the production team to review their work on both conventional displays and in HDR to ensure the range of image values is technically accurate and creatively consistent in both formats.
There are situations where some of the deliverables have a different artistic intent to the master, such as when a UHD HDR deliverable created after sign off on an SDR master. Because this deliverable significantly increases the range of colors beyond what was available during the initial digital cinema grade, a decision can be made to either create the HDR deliverable through a trim that stretches the original grade into the larger color volume of HDR or adjustments can be made in the grade itself to more fully utilize the additional information inherent in the source images. If a LUT has been used to apply the creative look for SDR, it may also be swapped out to an HDR look during this stage. The HDR format can reveal compromises in the source material that were not visible in the SDR master and require other corrections needed specifically for HDR. This needs to be considered to ensure enough time is allocated for HDR finishing.

The motion-picture viewing environment greatly impacts color appearance.  Theatrical viewing environments typically have dark surround and a relatively low screen luminance (14 fl, 48 cd/m2 is specified by DCI). In a desktop/office setting, an average surround is likely. Some common calibration targets demonstrate the variety of environment/ display combinations. 

	Display			Environment		Screen luminance	Gamma
Theatrical digital projection 	 dark surround		14 fl = 48 cd/m2  	2.6
Home digital projection 	 dark surround		16 fl = 55 cd/m2    	2.4
Reference TV monitor 	 dim surround		29 fl = 100 cd/m2 	2.4
Home TV monitor 	 	 average surround	50 fl = 170 cd/m2 	2.2
HDR10 monitor		 average surround	292 fl = 1000 cd/m2 	PQ

Thus, the same image displayed in each environment will appear very different. Specifically, the perceived image contrast and colorfulness will vary. Color appearance models may be used to attempt to correct for these settings, but trim passes with a human in the loop typically yield far higher fidelity results.

When mastering for stereoscopic 3D, an additional 3D specific trim pass is necessary. Due to the additional optics necessary to create the stereo image pair including additional filters in the projector and glasses on the audience, the projected stereo image is usually far lower luminance than traditional 2D theatrical projection. Early projection systems were limited to levels as low as 3.5 fl. As lower luminance images tend to appear less colorful, the trim passes for stereo masters typically boost both the saturation and contrast relative to the 2D master. However, this boost can impose a cost on image detail, for example clipping highlights. Also, the increased saturation and contrast will not be suitable for projection at significantly higher luminance. As projection technology has matured to support higher luminance projection this has led to the creation of multiple 3D deliverables. One way of managing this is to create a trim with boosted color for standard 3D venues, and a full brightness 3D grade without any color boost for venues that are capable of projecting at or close to 14 fl.

\subsection{Multiple deliverables}

There are two different approaches to multiple format deliveries; one is that all deliverables should look the same or at least as close as possible. Clearly, making all decisions in the smallest color space (Rec. 709) makes it straightforward to deliver in all other formats. Alternatively, making decisions in the largest available color space will mean that other deliverables may lack some of the color or brightness of the master, but the overall quality is likely to be better. For example, the Dolby Vision workflow gives the DI colorist responsibility for all deliverables using scene by scene metadata, that effectively extends control over the image, all the way to the playback device. Similar decisions have to be made for resolution and aspect ratio.

\subsubsection{Resolution}

The common resolutions for home deliverables are SD for DVD releases, HD for broadcast, Blu-ray and streaming, and UHD for newer channels including VOD and Blu-ray. Theatrical releases may have different aspect ratios but are also spread approximately between 2k and 4k resolutions. It is sensible for most production and post-production to target a 4k or better finishing pipeline. Editing can usually use lower resolution proxies, but full resolution files should be checked before making decisions. For wholly computer generated sources such as animated features there is a debate about whether 4k resolutions are needed.

\section{Archives}

Like color management, an archival procedure is relatively straightforward if it is included in planning from the start and incredibly traumatic if it has to be implemented after everything else. In a planned environment, archiving is little more than good housekeeping but the cost of introducing it retroactively can often be prohibitive.  One of the initial driving forces behind ACES was the lack of standards around color in archiving, and The Academy have recently published their Academy Digital Source Master draft specification, with the aim of standardizing the delivery archiving of ACES projects.

Project managers should consider how long the archive needs to last and to what purpose. The duration might be until the production is complete, delivered and fully signed off with the intention that even if a team has moved on to other things, all elements could be restored and work resumed should the need arise. This archive is separate from the day to day storage used by each department or facility, it is an untouched copy that is available if data is lost, corrupted or inaccessible. Alternatively, the archive could be part of a comprehensive asset management system intended to protect and future-proof valuable content. In theory, both scenarios face the same problems, but the solutions for risk management will differ in practice.

The challenges associated with archives can be simply stated as:
preservation of the data
preservation of the means to access the data
ease of access
low maintenance

As a rule, the best-preserved data is the least easy to access. A painting locked in a bank vault has a better chance of surviving intact than a painting on the wall of a cafe. Similarly, the safest digital files are those that do not have direct access by a host of users. Access to the original digital data requires a physical reader device, a compatible operating system with suitable software and the necessary ancillary support to view or use the data correctly. Within the concerns of this paper, that includes a display calibrated to the correct color gamut or the means to create a correct transform to interpret the images as intended. There should also be a file, document or metadata that specifies the correct color gamut. The metadata for HDR10 and Dolby Vision requires this but other formats assume that a display gamut. The Thomson Viper camera is a good long-term example of the pitfalls for archives. It was one of the first cameras to capture data rather than video or film. Much of the media shot on it was stored on videotape and required proprietary LUTs to correctly remove the inherent green bias. In the decade since the camera was introduced, those VTRs have become scarce. To retrieve Viper data, the tapes would have to have been well stored and you would have to locate a VTR. Additionally, there are no Input Transforms to bring Viper Filmstream data into an ACES workflow.

Preserving lossless copies of the digital data requires an ongoing managed schedule of refreshing, migrating and replicating the data. 

Refreshing is the transfer of data between two types of the same storage medium to protect against corruption or drive failure. This requires at least three copies so that any time one becomes different to the others, it is replaced. Checksum hashes should be calculated from the original media, and stored alongside each copy, so that its integrity can be subsequently verified. Even the loss of one bit can render an entire file unusable and there is a condition known as bit-rot that is statistically relevant to larger libraries. When an archive reaches a certain size some bit-rot is certain, although where it occurs is not. However, the highest risk to digital data is at the time of access and is caused either by operator error, or accidental deletion. For this reason, most archives do not allow access to the source content, they only allow content to be copied out of the archive. 

Duplicating the data is called Replication and of course, the more copies that exist, the less an asset is vulnerable to being lost. In the days of film, every release print made was a form of replication that increased the chances of an asset surviving. Unfortunately, however, there is a conflict of interest when an asset has commercial value. A studio that derives its income from sales of its content does not want countless copies in the market and therefore needs to plan and fund replication of high-quality master copies. When DVD and Rec. 709 broadcast were the biggest markets, studios archived to HD. As SDR HD has become more omnipresent, studios are moving to HDR UHD mastering. A few are already storing scene-referred copies of the graded and ungraded DI Masters with metadata that manages versioning for sound, picture format, length, and localization. This impacts the production and post-production pipeline since deliverables that are immediately compatible with the archive are likely to be a contractual requirement and that could narrow the workflow options. Replication and refreshing should include geo-redundant sites to avoid total failure in the event of a fire or similar catastrophe.

Short term archival needs might be covered by refreshing and replicating, but long-term preservation also requires planned migration. Migration is the transfer of data to newer technologies. That could be a new file format, an updated operating system or different storage media.  The move from analog to digital and the move from tape to file delivery are both examples of migration. The cost of migration for an archive can be very high, so it is good practice to future proof the assets as much as possible. Working scene-referred and keeping a scene-referred master saves very expensive re-mastering later and gives content a longer shelf life. However, migration of storage media, operating systems and file formats still apply. Often visual effects, editors and colorists save projects as part of the archive backup, but these have a relatively limited usefulness as software evolves and unless changes are requested projects are rarely migrated.

\subsection{Open Archives Information System (OAIS)}

In the 1970’s NASA recorded vast amounts of data from its space program and had an urgent need to develop a standard for practical long-term preservation of digital files. The resulting OAIS has since been the foundation for the digital archives of governments, libraries and content archives, large and small, around the world. The latest version is the Magenta Book of 2012 which describes the recommended practice for an organization of people and systems to protect and make available digital information. The word “Open” in the title refers to transparency of the system rather than meaning that access to the information should be unrestricted. It has been ratified as ISO 14721:2012.

The model uses the following terms for its components:
Producers create content and are production and post-production for purposes of this paper
Archive is the physical data storage of completed material, not work in progress.
Managers are those responsible for the policies that govern the Archive
Consumers are those that have legitimate access to the Archive content, not the end user.


OAIS Archive Model

It defines an Information Object as the product of a Data Object plus its Representation Information.
Data Object is a digital file, for example, an image file.
Representation Information is everything needed to understand that file. In the example of an image file that must include the file format, how to read that format and also everything required to display the file correctly.
Information Object is content that is preserved in the archive. Without the representation information, the data is at risk of being lost since there is an unreasonable assumption that the Knowledge Base for reading and understanding the information will always be universally known. The recommended practice likens this to a simple English text, without a suitable dictionary and grammar explanation.

OAIS Information Object

The simple information object is moved in and out of the archive and stored there as an Information Package, which consists of Content Information and Preservation Description Information (PDI). There needs to be some Packaging Information that keeps these two elements together and Descriptive Information that describes what the Information Package contains. The Descriptive Information could be as simple as a file name, or a more complicated list of delivery specifications.

Information Package

The PDI clearly identifies the Content Information and the model goes on to identify five types of information that should be recorded.
	Provenance describes the source, history, and ownership of the material
	Context describes any relationships with other content, for example, an episode in a series. It might contain other relationships such as the originating studio, producer or director.
	Reference information should include any unique identifiers that are or could be used to recognize the content in one or more systems
	Fixity is protection from undocumented alteration and could be in the form of a checksum or other method of ensuring that the information is as described
	Access Rights contains the terms of access, ownership, licensing, and permission for preservation operations as well as specifications for rights enforcement and access control.

The Reference Model for an Open Archival Information System Magenta Book goes into a lot more detail on setting up, managing and using an archive, which is beyond the scope of this paper. This basic introduction to data management is however relevant at all stages of data production, manipulation and delivery and is worth bearing in mind when sharing digital files between software, departments or organizations.

\subsection{Archive Elements}

The primary archival asset for motion picture projects is the master deliverable or deliverables since these are ready for use as long as their formats are current. File names and/or metadata should clearly identify the intended display description including the primaries, the EOTF and the white point. HDR10 and Dolby Vision formats include this information in their metadata. With DCP and IMF packages there can be a single master with alternative scenes and soundtracks included in the package, together with metadata to generate other versions. 

Some content owners demand an ungraded master for archiving. This is more useful if the DI project is also available, though the content owner is rarely the keeper of any project files. Projects for DI grades, editing or VFX are difficult to preserve intact. Software and hardware changes eventually  leave projects unusable, so a good archive will keep the software version, operating system and hardware if it needs to recover projects. The project will also need the source files to be fully usable and many facilities use a fixed folder structure for each project to make a full backup easier.
